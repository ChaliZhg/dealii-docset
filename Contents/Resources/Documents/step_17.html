<!-- HTML header for doxygen 1.8.13-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>The deal.II Library: The step-17 tutorial program</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link rel="SHORTCUT ICON" href="deal.ico"></link>
<script type="text/javascript" src="custom.js"></script>
<meta name="author" content="The deal.II Authors <authors@dealii.org>"></meta>
<meta name="copyright" content="Copyright (C) 1998 - 2017 by the deal.II authors"></meta>
<meta name="deal.II-version" content="9.0.0-pre"></meta>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="logo200.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">
   &#160;<span id="projectnumber">Reference documentation for deal.II version 9.0.0-pre</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">The step-17 tutorial program </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p> 
<table class="tutorial" width="50%">
<tr><th colspan="2"><b><small>Table of contents</small></b></th></tr>
<tr><td width="50%" valign="top">
<ol>
  <li> <a href="#Intro" class=bold>Introduction</a>
    <ul>
    </ul>
  <li> <a href="#CommProg" class=bold>The commented program</a>
    <ul>
        <li><a href="#Includefiles">Include files</a>
        <li><a href="#ThecodeElasticProblemcodeclasstemplate">The <code>ElasticProblem</code> class template</a>
        <li><a href="#Righthandsidevalues">Right hand side values</a>
        <li><a href="#ThecodeElasticProblemcodeclassimplementation">The <code>ElasticProblem</code> class implementation</a>
      <ul>
        <li><a href="#ElasticProblemElasticProblem">ElasticProblem::ElasticProblem</a>
        <li><a href="#ElasticProblemElasticProblem">ElasticProblem::~ElasticProblem</a>
        <li><a href="#ElasticProblemsetup_system">ElasticProblem::setup_system</a>
        <li><a href="#ElasticProblemassemble_system">ElasticProblem::assemble_system</a>
        <li><a href="#ElasticProblemsolve">ElasticProblem::solve</a>
        <li><a href="#ElasticProblemrefine_grid">ElasticProblem::refine_grid</a>
        <li><a href="#ElasticProblemoutput_results">ElasticProblem::output_results</a>
        <li><a href="#ElasticProblemrun">ElasticProblem::run</a>
      </ul>
        <li><a href="#Thecodemaincodefunction">The <code>main</code> function</a>
      </ul>
</ol></td><td width="50%" valign="top"><ol>
  <li value="3"> <a href="#Results" class=bold>Results</a>
    <ul>
        <li><a href="#Possibilitiesforextensions">Possibilities for extensions</a>
    </ul>
  <li> <a href="#PlainProg" class=bold>The plain program</a>
</ol> </td> </tr> </table>
 <a class="anchor" id="Intro"></a> <a class="anchor" id="Introduction"></a></p><h1>Introduction</h1>
<p><a class="anchor" id="Overview"></a></p><h2>Overview</h2>
<p>This program does not introduce any new mathematical ideas; in fact, all it does is to do the exact same computations that <a class="el" href="step_8.html">step-8</a> already does, but it does so in a different manner: instead of using deal.II's own linear algebra classes, we build everything on top of classes deal.II provides that wrap around the linear algebra implementation of the <a href="http://www.mcs.anl.gov/petsc/" target="_top">PETSc</a> library. And since PETSc allows to distribute matrices and vectors across several computers within an MPI network, the resulting code will even be able to solve the problem in parallel. If you don't know what PETSc is, then this would be a good time to take a quick glimpse at their homepage.</p>
<p>As a prerequisite of this program, you need to have PETSc installed, and if you want to run in parallel on a cluster, you also need <a href="http://www-users.cs.umn.edu/~karypis/metis/index.html" target="_top">METIS</a> to partition meshes. The installation of deal.II together with these two additional libraries is described in the <a href="https://www.dealii.org/developer/readme.html" target="body">README</a> file.</p>
<p>Now, for the details: as mentioned, the program does not compute anything new, so the use of finite element classes, etc., is exactly the same as before. The difference to previous programs is that we have replaced almost all uses of classes <code><a class="el" href="classVector.html">Vector</a></code> and <code><a class="el" href="classSparseMatrix.html">SparseMatrix</a></code> by their near-equivalents <code><a class="el" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a></code> and <code><a class="el" href="classPETScWrappers_1_1MPI_1_1SparseMatrix.html">PETScWrappers::MPI::SparseMatrix</a></code> that store data in a way so that every processor in the MPI network only stores a part of the matrix or vector. More specifically, each processor will only store those rows of the matrix that correspond to a degree of freedom it "owns". For vectors, they either store only elements that correspond to degrees of freedom the processor owns (this is what is necessary for the right hand side), or also some additional elements that make sure that every processor has access the solution components that live on the cells the processor owns (so-called <a class="el" href="DEALGlossary.html#GlossLocallyActiveDof">locally active DoFs</a>) or also on neighboring cells (so-called <a class="el" href="DEALGlossary.html#GlossLocallyRelevantDof">locally relevant DoFs</a>).</p>
<p>The interface the classes from the PETScWrapper namespace provide is very similar to that of the deal.II linear algebra classes, but instead of implementing this functionality themselves, they simply pass on to their corresponding PETSc functions. The wrappers are therefore only used to give PETSc a more modern, object oriented interface, and to make the use of PETSc and deal.II objects as interchangeable as possible. The main point of using PETSc is that it can run in parallel. We will make use of this by partitioning the domain into as many blocks ("subdomains") as there are processes in the MPI network. At the same time, PETSc also provides dummy MPI stubs, so you can run this program on a single machine if PETSc was configured without MPI.</p>
<p><a class="anchor" id="ParallelizingsoftwarewithMPI"></a></p><h2>Parallelizing software with MPI</h2>
<p>Developing software to run in parallel via MPI requires a bit of a change in mindset because one typically has to split up all data structures so that every processor only stores a piece of the entire problem. As a consequence, you can't typically access all components of a solution vector on each processor &ndash; each processor may simply not have enough memory to hold the entire solution vector. Because data is split up or "distributed" across processors, we call the programming model used by MPI "distributed memory
computing" (as opposed to "shared memory computing", which would mean that multiple processors can all access all data within one memory space, for example whenever multiple cores in a single machine work on a common task). Some of the fundamentals of distributed memory computing are discussed in the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using distributed memory</a> documentation module, which is itself a sub-module of the <a class="el" href="group__Parallel.html">Parallel computing</a> module.</p>
<p>In general, to be truly able to scale to large numbers of processors, one needs to split between the available processors <em>every</em> data structure whose size scales with the size of the overall problem. (For a definition of what it means for a program to "scale", see <a class="el" href="DEALGlossary.html#GlossParallelScaling">this glossary entry.) This includes, for</a>example, the triangulation, the matrix, and all global vectors (solution, right hand side). If one doesn't split all of these objects, one of those will be replicated on all processors and will eventually simply become too large if the problem size (and the number of available processors) becomes large. (On the other hand, it is completely fine to keep objects with a size that is independent of the overall problem size on every processor. For example, each copy of the executable will create its own finite element object, or the local matrix we use in the assembly.)</p>
<p>In the current program (as well as in the related <a class="el" href="step_18.html">step-18</a>), we will not go quite this far but present a gentler introduction to using MPI. More specifically, the only data structures we will parallelize are matrices and vectors. We do, however, not split up the <a class="el" href="classTriangulation.html">Triangulation</a> and <a class="el" href="classDoFHandler.html">DoFHandler</a> classes: each process still has a complete copy of these objects, and all processes have exact copies of what the other processes have. We will then simply have to mark, in each copy of the triangulation on each of the processors, which processor owns which cells. This process is called "partitioning" a mesh into <a class="el" href="DEALGlossary.html#GlossSubdomainId">subdomains</a>.</p>
<p>For larger problems, having to store the <em>entire</em> mesh on every processor will clearly yield a bottleneck. Splitting up the mesh is slightly, though not much more complicated (from a user perspective, though it is <em>much</em> more complicated under the hood) to achieve and we will show how to do this in <a class="el" href="step_40.html">step-40</a> and some other programs. There are numerous occasions where, in the course of discussing how a function of this program works, we will comment on the fact that it will not scale to large problems and why not. All of these issues will be addressed in <a class="el" href="step_18.html">step-18</a> and in particular <a class="el" href="step_40.html">step-40</a>, which scales to very large numbers of processes.</p>
<p>Philosophically, the way MPI operates is as follows. You typically run a program via </p><div class="fragment"><div class="line">mpirun -np 32 ./step-17</div></div><!-- fragment --><p> which means to run it on (say) 32 processors. (If you are on a cluster system, you typically need to <em>schedule</em> the program to run whenever 32 processors become available; this will be described in the documentation of your cluster. But under the hood, whenever those processors become available, the same call as above will generally be executed.) What this does is that the MPI system will start 32 <em>copies</em> of the <code><a class="el" href="step_17.html">step-17</a></code> executable. (The MPI term for each of these running executables is that you have 32 <a class="el" href="DEALGlossary.html#GlossMPIProcess">MPI processes</a>.) This may happen on different machines that can't even read from each others' memory spaces, or it may happen on the same machine, but the end result is the same: each of these 32 copies will run with some memory allocated to it by the operating system, and it will not directly be able to read the memory of the other 31 copies. In order to collaborate in a common task, these 32 copies then have to <em>communicate</em> with each other. MPI, short for <em>Message Passing Interface</em>, makes this possible by allowing programs to <em>send messages</em>. You can think of this as the mail service: you can put a letter to a specific address into the mail and it will be delivered. But that's the extent to which you can control things. If you want the receiver to do something with the content of the letter, for example return to you data you want from over there, then two things need to happen: (i) the receiver needs to actually go check whether there is anything in her mailbox, and (ii) if there is, react appropriately, for example by sending data back. If you wait for this return message but the original receiver was distracted and not paying attention, then you're out of luck: you'll simply have to wait until your requested over there will be worked on. In some cases, bugs will lead the original receiver to never check your mail, and in that case you will wait forever &ndash; this is called a <em>deadlock</em>. (See also <a href="http://www.math.colostate.edu/~bangerth/videos.676.39.html">video lecture 39</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.html">video lecture 41</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.25.html">video lecture 41.25</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.5.html">video lecture 41.5</a>.)</p>
<p>In practice, one does not usually program at the level of sending and receiving individual messages, but uses higher level operations. For example, in the program we will use function calls that take a number from each processor, add them all up, and return the sum to all processors. Internally, this is implemented using individual messages, but to the user this is transparent. We call such operations <em>collectives</em> because <em>all</em> processors participate in them. Collectives allow us to write programs where not every copy of the executable is doing something completely different (this would be incredibly difficult to program) but where in essence all copies are doing the same thing (though on different data) for themselves, running through the same blocks of code; then they communicate data through collectives; and then go back to doing something for themselves again running through the same blocks of data. This is the key piece to being able to write programs, and it is the key component to making sure that programs can run on any number of processors, since we do not have to write different code for each of the participating processors.</p>
<p>(This is not to say that programs are never written in ways where different processors run through different blocks of code in their copy of the executable. Programs internally also often communicate in other ways than through collectives. But in practice, parallel finite finite element codes almost always follow the scheme where every copy of the program runs through the same blocks of code at the same time, interspersed by phases where all processors communicate with each other.)</p>
<p>In reality, even the level of calling MPI collective functions is too low. Rather, the program below will not contain any direct calls to MPI at all, but only deal.II functions that hide this communication from users of the deal.II. This has the advantage that you don't have to learn the details of MPI and its rather intricate function calls. That said, you do have to understand the general philosophy behind MPI as outlined above.</p>
<p><a class="anchor" id="Whatthisprogramdoes"></a></p><h2>What this program does</h2>
<p>The techniques this program then demonstrates are:</p><ul>
<li>How to use the PETSc wrapper classes; this will already be visible in the declaration of the principal class of this program, <code>ElasticProblem</code>.</li>
<li>How to partition the mesh into subdomains; this happens in the <code>ElasticProblem::setup_system()</code> function.</li>
<li>How to parallelize operations for jobs running on an MPI network; here, this is something one has to pay attention to in a number of places, most notably in the <code>ElasticProblem::assemble_system()</code> function.</li>
<li>How to deal with vectors that store only a subset of vector entries and for which we have to ensure that they store what we need on the current processors. See for example the <code>ElasticProblem::solve()</code> and <code>ElasticProblem::refine_grid()</code> functions.</li>
<li>How to deal with status output from programs that run on multiple processors at the same time. This is done via the <code>pcout</code> variable in the program, initialized in the constructor.</li>
</ul>
<p>Since all this can only be demonstrated using actual code, let us go straight to the code without much further ado.</p>
<p><a class="anchor" id="CommProg"></a> </p><h1>The commented program</h1>
<p><a class="anchor" id="Includefiles"></a> </p><h3>Include files</h3>
<p>First the usual assortment of header files we have already used in previous example programs:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;deal.II/base/quadrature_lib.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/base/function.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/base/logstream.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/base/multithread_info.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/vector.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/full_matrix.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/constraint_matrix.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/dynamic_sparsity_pattern.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/sparsity_tools.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/tria.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/grid_generator.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/grid_refinement.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/tria_accessor.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/tria_iterator.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/tria_boundary_lib.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/dofs/dof_handler.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/dofs/dof_accessor.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/dofs/dof_tools.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/fe/fe_values.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/fe/fe_system.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/fe/fe_q.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/numerics/vector_tools.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/numerics/matrix_tools.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/numerics/data_out.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/numerics/error_estimator.h&gt;</span></div></div><!-- fragment --><p>And here come the things that we need particularly for this example program and that weren't in <a class="el" href="step_8.html">step-8</a>. First, we replace the standard output <code>std::cout</code> by a new stream <code>pcout</code> which is used in parallel computations for generating output only on one of the MPI processes.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;deal.II/base/conditional_ostream.h&gt;</span></div></div><!-- fragment --><p>We are going to query the number of processes and the number of the present process by calling the respective functions in the <a class="el" href="namespaceUtilities_1_1MPI.html">Utilities::MPI</a> namespace.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;deal.II/base/mpi.h&gt;</span></div></div><!-- fragment --><p>Then, we are going to replace all linear algebra components that involve the (global) linear system by classes that wrap interfaces similar to our own linear algebra classes around what PETSc offers (PETSc is a library written in C, and deal.II comes with wrapper classes that provide the PETSc functionality with an interface that is similar to the interface we already had for our own linear algebra classes). In particular, we need vectors and matrices that are distributed across several <a class="el" href="DEALGlossary.html#GlossMPIProcess">processes</a> in MPI programs (and simply map to sequential, local vectors and matrices if there is only a single process, i.e. if you are running on only one machine, and without MPI support):</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/petsc_parallel_vector.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/petsc_parallel_sparse_matrix.h&gt;</span></div></div><!-- fragment --><p>Then we also need interfaces for solvers and preconditioners that PETSc provides:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/petsc_solver.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/petsc_precondition.h&gt;</span></div></div><!-- fragment --><p>And in addition, we need some algorithms for partitioning our meshes so that they can be efficiently distributed across an MPI network. The partitioning algorithm is implemented in the <code><a class="el" href="namespaceGridTools.html">GridTools</a></code> namespace, and we need an additional include file for a function in <code><a class="el" href="namespaceDoFRenumbering.html">DoFRenumbering</a></code> that allows to sort the indices associated with degrees of freedom so that they are numbered according to the subdomain they are associated with:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/grid_tools.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/dofs/dof_renumbering.h&gt;</span></div></div><!-- fragment --><p>And this is simply C++ again:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;sstream&gt;</span></div></div><!-- fragment --><p>The last step is as in all previous programs:</p>
<div class="fragment"><div class="line"><span class="keyword">namespace </span>Step17</div><div class="line">{</div><div class="line">  <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div></div><!-- fragment --><p><a class="anchor" id="ThecodeElasticProblemcodeclasstemplate"></a> </p><h3>The <code>ElasticProblem</code> class template</h3>
<p>The first real part of the program is the declaration of the main class. As mentioned in the introduction, almost all of this has been copied verbatim from <a class="el" href="step_8.html">step-8</a>, so we only comment on the few differences between the two tutorials. There is one (cosmetic) change in that we let <code>solve</code> return a value, namely the number of iterations it took to converge, so that we can output this to the screen at the appropriate place.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keyword">class </span>ElasticProblem</div><div class="line">{</div><div class="line"><span class="keyword">public</span>:</div><div class="line">  ElasticProblem ();</div><div class="line">  ~ElasticProblem ();</div><div class="line">  <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream.html#a0c5d332d74a4df80784140218896b169">run</a> ();</div><div class="line"></div><div class="line"><span class="keyword">private</span>:</div><div class="line">  <span class="keywordtype">void</span> setup_system ();</div><div class="line">  <span class="keywordtype">void</span> assemble_system ();</div><div class="line">  <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> solve ();</div><div class="line">  <span class="keywordtype">void</span> refine_grid ();</div><div class="line">  <span class="keywordtype">void</span> output_results (<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div></div><!-- fragment --><p>The first change is that we have to declare a variable that indicates the <a class="el" href="DEALGlossary.html#GlossMPICommunicator">MPI communicator</a> over which we are supposed to distribute our computations.</p>
<div class="fragment"><div class="line">MPI_Comm mpi_communicator;</div></div><!-- fragment --><p>Then we have two variables that tell us where in the parallel world we are. The first of the following variables, <code>n_mpi_processes</code>, tells us how many MPI processes there exist in total, while the second one, <code>this_mpi_process</code>, indicates which is the number of the present process within this space of processes (in MPI language, this corresponds to the <a class="el" href="DEALGlossary.html#GlossMPIRank">rank</a> of the process). The latter will have a unique value for each process between zero and (less than) <code>n_mpi_processes</code>. If this program is run on a single machine without MPI support, then their values are <code>1</code> and <code>0</code>, respectively.</p>
<div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>;</div><div class="line"><span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>;</div></div><!-- fragment --><p>Next up is a stream-like variable <code>pcout</code>. It is, in essence, just something we use for convenience: in a parallel program, if each process outputs status information, then there quickly is a lot of clutter. Rather, we would want to only have one <a class="el" href="DEALGlossary.html#GlossMPIProcess">process</a> output everything once, for example the one with <a class="el" href="DEALGlossary.html#GlossMPIRank">rank</a> zero. At the same time, it seems silly to prefix <em>every</em> places where we create output with an <code>if (my_rank==0)</code> condition.</p>
<p>To make this simpler, the <a class="el" href="classConditionalOStream.html">ConditionalOStream</a> class does exactly this under the hood: it acts as if it were a stream, but only forwards to a real, underlying stream if a flag is set. By setting this condition to <code>this_mpi_process==0</code> (where <code>this_mpi_process</code> corresponds to the rank of an MPI process), we make sure that output is only generated from the first process and that we don't get the same lines of output over and over again, once per process. Thus, we can use <code>pcout</code> everywhere and in every process, but on all but one process nothing will ever happen to the information that is piped into the object via <code>operator&lt;&lt;</code>.</p>
<div class="fragment"><div class="line"><a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div></div><!-- fragment --><p>The remainder of the list of member variables is fundamentally the same as in <a class="el" href="step_8.html">step-8</a>. However, we change the declarations of matrix and vector types to use parallel PETSc objects instead. Note that we do not use a separate sparsity pattern, since PETSc manages this internally as part of its matrix data structures.</p>
<div class="fragment"><div class="line">  <a class="code" href="classTriangulation.html">Triangulation&lt;dim&gt;</a>   triangulation;</div><div class="line">  <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a>      dof_handler;</div><div class="line"></div><div class="line">  <a class="code" href="classFESystem.html">FESystem&lt;dim&gt;</a>        fe;</div><div class="line"></div><div class="line">  <a class="code" href="classConstraintMatrix.html">ConstraintMatrix</a>     hanging_node_constraints;</div><div class="line"></div><div class="line">  <a class="code" href="classPETScWrappers_1_1MPI_1_1SparseMatrix.html">PETScWrappers::MPI::SparseMatrix</a> system_matrix;</div><div class="line"></div><div class="line">  <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a>       solution;</div><div class="line">  <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a>       system_rhs;</div><div class="line">};</div></div><!-- fragment --><p><a class="anchor" id="Righthandsidevalues"></a> </p><h3>Right hand side values</h3>
<p>The following is taken from <a class="el" href="step_8.html">step-8</a> without change:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keyword">class </span>RightHandSide :  <span class="keyword">public</span> <a class="code" href="classFunction.html">Function</a>&lt;dim&gt;</div><div class="line">{</div><div class="line"><span class="keyword">public</span>:</div><div class="line">  RightHandSide ();</div><div class="line"></div><div class="line">  <span class="keyword">virtual</span> <span class="keywordtype">void</span> <a class="code" href="classFunction.html#ab82f495e6e2f2cc59b7173a2d804e986">vector_value</a> (<span class="keyword">const</span> <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &amp;p,</div><div class="line">                             <a class="code" href="classVector.html">Vector&lt;double&gt;</a>   &amp;values) <span class="keyword">const</span>;</div><div class="line"></div><div class="line">  <span class="keyword">virtual</span> <span class="keywordtype">void</span> <a class="code" href="classFunction.html#affaffa06986e55c66f71b8117087a0b6">vector_value_list</a> (<span class="keyword">const</span> std::vector&lt;<a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &gt; &amp;points,</div><div class="line">                                  std::vector&lt;<a class="code" href="classVector.html">Vector&lt;double&gt;</a> &gt;   &amp;value_list) <span class="keyword">const</span>;</div><div class="line">};</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">RightHandSide&lt;dim&gt;::RightHandSide () :</div><div class="line">  <a class="code" href="classFunction.html">Function</a>&lt;dim&gt; (dim)</div><div class="line">{}</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keyword">inline</span></div><div class="line"><span class="keywordtype">void</span> RightHandSide&lt;dim&gt;::vector_value (<span class="keyword">const</span> <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &amp;p,</div><div class="line">                                       <a class="code" href="classVector.html">Vector&lt;double&gt;</a>   &amp;values)<span class="keyword"> const</span></div><div class="line"><span class="keyword"></span>{</div><div class="line">  <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a> (values.<a class="code" href="classVector.html#a8005bf1ec399c608c3755c1d22960add">size</a>() == dim,</div><div class="line">          <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a> (values.<a class="code" href="classVector.html#a8005bf1ec399c608c3755c1d22960add">size</a>(), dim));</div><div class="line">  <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a> (dim &gt;= 2, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line"></div><div class="line">  <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> point_1, point_2;</div><div class="line">  point_1(0) = 0.5;</div><div class="line">  point_2(0) = -0.5;</div><div class="line"></div><div class="line">  <span class="keywordflow">if</span> (((p-point_1).norm_square() &lt; 0.2*0.2) ||</div><div class="line">      ((p-point_2).norm_square() &lt; 0.2*0.2))</div><div class="line">    values(0) = 1;</div><div class="line">  <span class="keywordflow">else</span></div><div class="line">    values(0) = 0;</div><div class="line"></div><div class="line">  <span class="keywordflow">if</span> (p.<a class="code" href="classPoint.html#a859ea7f3bf3e64be2e0f5ed1bfcc8550">square</a>() &lt; 0.2*0.2)</div><div class="line">    values(1) = 1;</div><div class="line">  <span class="keywordflow">else</span></div><div class="line">    values(1) = 0;</div><div class="line">}</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> RightHandSide&lt;dim&gt;::vector_value_list (<span class="keyword">const</span> std::vector&lt;<a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &gt; &amp;points,</div><div class="line">                                            std::vector&lt;<a class="code" href="classVector.html">Vector&lt;double&gt;</a> &gt;   &amp;value_list)<span class="keyword"> const</span></div><div class="line"><span class="keyword"></span>{</div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_points = points.<a class="code" href="classVector.html#a8005bf1ec399c608c3755c1d22960add">size</a>();</div><div class="line"></div><div class="line">  <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a> (value_list.size() == n_points,</div><div class="line">          <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a> (value_list.size(), n_points));</div><div class="line"></div><div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p=0; p&lt;n_points; ++p)</div><div class="line">    RightHandSide&lt;dim&gt;::vector_value (points[p],</div><div class="line">                                      value_list[p]);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ThecodeElasticProblemcodeclassimplementation"></a> </p><h3>The <code>ElasticProblem</code> class implementation</h3>
<p><a class="anchor" id="ElasticProblemElasticProblem"></a> </p><h4>ElasticProblem::ElasticProblem</h4>
<p>The first step in the actual implementation is the constructor of the main class. Apart from initializing the same member variables that we already had in <a class="el" href="step_8.html">step-8</a>, we here initialize the MPI communicator variable we shall use with the global MPI communicator linking all processes together (in more complex applications, one could here use a communicator object that only links a subset of all processes), and call the <a class="el" href="namespaceUtilities_1_1MPI.html">Utilities::MPI</a> helper functions to determine the number of processes and where the present one fits into this picture. In addition, we make sure that output is only generated by the (globally) first process. We do so by passing the stream we want to output to (<code>std::cout</code>) and a true/false flag as arguments where the latter is determined by testing whether the process currently executing the constructor call is the first in the MPI universe.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">ElasticProblem&lt;dim&gt;::ElasticProblem ()</div><div class="line">  :</div><div class="line">  mpi_communicator (MPI_COMM_WORLD),</div><div class="line">  <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a> (<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(mpi_communicator)),</div><div class="line">  <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> (<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator)),</div><div class="line">  pcout (<a class="code" href="namespacestd.html">std</a>::cout,</div><div class="line">         (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0)),</div><div class="line">  dof_handler (triangulation),</div><div class="line">  fe (<a class="code" href="classFE__Q.html">FE_Q</a>&lt;dim&gt;(1), dim)</div><div class="line">{}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemElasticProblem"></a> </p><h4>ElasticProblem::~ElasticProblem</h4>
<p>The destructor is exactly as in <a class="el" href="step_8.html">step-8</a>.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">ElasticProblem&lt;dim&gt;::~ElasticProblem ()</div><div class="line">{</div><div class="line">  dof_handler.<a class="code" href="classDoFHandler.html#ad316958f8045d9a48094335b23a03a53">clear</a> ();</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemsetup_system"></a> </p><h4>ElasticProblem::setup_system</h4>
<p>Next, the function in which we set up the various variables for the global linear system to be solved needs to be implemented.</p>
<p>However, before we with this, there is one thing to do for a parallel program: we need to determine which MPI process is responsible for each of the cells. Splitting cells among processes, commonly called "partitioning the mesh", is done by assigning a <a class="el" href="DEALGlossary.html#GlossSubdomainId">subdomain id</a> to each cell. We do so by calling into the METIS library that does this in a very efficient way, trying to minimize the number of nodes on the interfaces between subdomains. Rather than trying to call METIS directly, we do this by calling the <a class="el" href="namespaceGridTools.html#a5b4706b77042db5437cf81fd13e62b20">GridTools::partition_triangulation()</a> function that does this at a much higher level of programming.</p>
<dl class="section note"><dt>Note</dt><dd>As mentioned in the introduction, we could avoid this manual partitioning step if we used the <a class="el" href="classparallel_1_1shared_1_1Triangulation.html">parallel::shared::Triangulation</a> class for the triangulation object instead (as we do in <a class="el" href="step_18.html">step-18</a>). That class does, in essence, everything a regular triangulation does, but it then also automatically partitions the mesh after every mesh creation or refinement operation.</dd></dl>
<p>Following partitioning, we need to enumerate all degrees of freedom as usual. However, we would like to enumerate the degrees of freedom in a way so that all degrees of freedom associated with cells in subdomain zero (which resides on process zero) come before all DoFs associated with cells on subdomain one, before those on cells on process two, and so on. We need this since we have to split the global vectors for right hand side and solution, as well as the matrix into contiguous chunks of rows that live on each of the processors, and we will want to do this in a way that requires minimal communication. This particular enumeration can be obtained by re-ordering degrees of freedom indices using <a class="el" href="namespaceDoFRenumbering.html#a442e0991d482f8d208069945d4cee508">DoFRenumbering::subdomain_wise()</a>.</p>
<p>The final step of this initial setup is that we get ourselves an <a class="el" href="classIndexSet.html">IndexSet</a> that indicates the subset of the global number of unknowns this process is responsible for. (Note that a degree of freedom is not necessarily owned by the process that owns a cell just because the degree of freedom lives on this cell: some degrees of freedom live on interfaces between subdomains, and are consequently only owned by one of the processes adjacent to this interface.)</p>
<p>Before we move on, let us recall a fact already discussed in the introduction: The triangulation we use here is replicated across all processes, and each process has a complete copy of the entire triangulation, with all cells. Partitioning only provides a way to identify which cells out of all each process "owns", but it knows everything about all of them. Likewise, the <a class="el" href="classDoFHandler.html">DoFHandler</a> object knows everything about every cell, in particular the degrees of freedom that live on each cell, whether it is one that the current process owns or not. This can not scale to large problems because eventually just storing on every process the entire mesh, and everything that is associated with it, will become infeasible if the problem is large enough. On the other hand, if we split the triangulation into parts so that every process stores only those cells it "owns" but nothing else (or, at least a sufficiently small fraction of everything else), then we can solve large problems if only we throw a large enough number of MPI processes at them. This is what we are going to in <a class="el" href="step_40.html">step-40</a>, for example, using the <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> class. On the other hand, most of the rest of what we demonstrate in the current program will actually continue to work whether we have the entire triangulation available, or only a piece of it.</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::setup_system ()</div><div class="line">{</div><div class="line">  <a class="code" href="namespaceGridTools.html#a5b4706b77042db5437cf81fd13e62b20">GridTools::partition_triangulation</a> (n_mpi_processes, triangulation);</div><div class="line"></div><div class="line">  dof_handler.<a class="code" href="classDoFHandler.html#a553ca864aaf70330d9be86bc78f36d1e">distribute_dofs</a> (fe);</div><div class="line">  <a class="code" href="namespaceDoFRenumbering.html#a442e0991d482f8d208069945d4cee508">DoFRenumbering::subdomain_wise</a> (dof_handler);</div></div><!-- fragment --><p>We need to initialize the objects denoting hanging node constraints for the present grid. As with the triangulation and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects, we will simply store <em>all</em> constraints on each process; again, this will not scale, but we show in <a class="el" href="step_40.html">step-40</a> how one can work around this by only storing on each MPI process the constraints for degrees of freedom that actually matter on this particular process.</p>
<div class="fragment"><div class="line">hanging_node_constraints.clear ();</div><div class="line"><a class="code" href="group__constraints.html#ga3eaa31a679484e80c193e74e8a967dc8">DoFTools::make_hanging_node_constraints</a> (dof_handler,</div><div class="line">                                         hanging_node_constraints);</div><div class="line">hanging_node_constraints.close ();</div></div><!-- fragment --><p>Now we create the sparsity pattern for the system matrix. Note that we again compute and store all entries and not only the ones relevant to this process (see <a class="el" href="step_18.html">step-18</a> or <a class="el" href="step_40.html">step-40</a> for a more efficient way to handle this).</p>
<div class="fragment"><div class="line"><a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>(), dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>());</div><div class="line"><a class="code" href="group__constraints.html#ga38d88a1a559e9fc65d60f3e168921ba5">DoFTools::make_sparsity_pattern</a> (dof_handler, dsp,</div><div class="line">                                 hanging_node_constraints,</div><div class="line">                                 <span class="keyword">false</span>);</div></div><!-- fragment --><p>Now we determine the set of locally owned DoFs and use that to initialize parallel vectors and matrix. Since the matrix and vectors need to work in parallel, we have to pass them an MPI communication object, as well as information about the partitioning contained in the <a class="el" href="classIndexSet.html">IndexSet</a> <code>locally_owned_dofs</code>. The <a class="el" href="classIndexSet.html">IndexSet</a> contains information about the global size (the <em>total</em> number of degrees of freedom) and also what subset of rows is to be stored locally. Note that the system matrix needs that partitioning information for the rows and columns. For square matrices, as it is the case here, the columns should be partitioned in the same way as the rows, but in the case of rectangular matrices one has to partition the columns in the same way as vectors are partitioned with which the matrix is multiplied, while rows have to partitioned in the same way as destination vectors of matrix-vector multiplications:</p>
<div class="fragment"><div class="line">  <span class="keyword">const</span> std::vector&lt;IndexSet&gt; locally_owned_dofs_per_proc = <a class="code" href="namespaceDoFTools.html#a6985533ce0fbf92a5a2027191e90c90b">DoFTools::locally_owned_dofs_per_subdomain</a>(dof_handler);</div><div class="line">  <span class="keyword">const</span> <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs = locally_owned_dofs_per_proc[<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>];</div><div class="line"></div><div class="line">  system_matrix.reinit (locally_owned_dofs,</div><div class="line">                        locally_owned_dofs,</div><div class="line">                        dsp,</div><div class="line">                        mpi_communicator);</div><div class="line"></div><div class="line">  solution.<a class="code" href="classVector.html#ac4a4dbef7dd65ef8ad35ae56b57d7c05">reinit</a> (locally_owned_dofs, mpi_communicator);</div><div class="line">  system_rhs.reinit (locally_owned_dofs, mpi_communicator);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemassemble_system"></a> </p><h4>ElasticProblem::assemble_system</h4>
<p>We now assemble the matrix and right hand side of the problem. There are some things worth mentioning before we go into detail. First, we will be assembling the system in parallel, i.e., each process will be responsible for assembling on cells that belong to this particular process. Note that the degrees of freedom are split in a way such that all DoFs in the interior of cells and between cells belonging to the same subdomain belong to the process that <code>owns</code> the cell. However, even then we sometimes need to assemble on a cell with a neighbor that belongs to a different process, and in these cases when we add up the local contributions into the global matrix or right hand side vector, we have to transfer these entries to the process that owns these elements. Fortunately, we don't have to do this by hand: PETSc does all this for us by caching these elements locally, and sending them to the other processes as necessary when we call the <code>compress()</code> functions on the matrix and vector at the end of this function.</p>
<p>The second point is that once we have handed over matrix and vector contributions to PETSc, it is a) hard, and b) very inefficient to get them back for modifications. This is not only the fault of PETSc, it is also a consequence of the distributed nature of this program: if an entry resides on another processor, then it is necessarily expensive to get it. The consequence of this is that we should not try to first assemble the matrix and right hand side as if there were no hanging node constraints and boundary values, and then eliminate these in a second step (using, for example, <a class="el" href="classConstraintMatrix.html#a05a5f8d313eb2c777e8c9a66b9cd0a62">ConstraintMatrix::condense()</a>). Rather, we should try to eliminate hanging node constraints before handing these entries over to PETSc. This is easy: instead of copying elements by hand into the global matrix (as we do in <a class="el" href="step_4.html">step-4</a>), we use the <a class="el" href="classConstraintMatrix.html#a1c61203741d499990c6288c3fcf3d48c">ConstraintMatrix::distribute_local_to_global()</a> functions to take care of hanging nodes at the same time. We also already did this in <a class="el" href="step_6.html">step-6</a>. The second step, elimination of boundary nodes, could also be done this way by putting the boundary values into the same <a class="el" href="classConstraintMatrix.html">ConstraintMatrix</a> object as hanging nodes (see the way it is done in <a class="el" href="step_6.html">step-6</a>, for example); however, it is not strictly necessary to do this here because eliminating boundary values can be done with only the data stored on each process itself, and consequently we use the approach used before in <a class="el" href="step_4.html">step-4</a>, i.e., via <a class="el" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values()</a>.</p>
<p>All of this said, here is the actual implementation starting with the general setup of helper variables. (Note that we still use the deal.II full matrix and vector types for the local systems as these are small and need not be shared across processes.)</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::assemble_system ()</div><div class="line">{</div><div class="line">  <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a>  quadrature_formula(2);</div><div class="line">  <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values (fe, quadrature_formula,</div><div class="line">                           <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a>   | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                           <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>   dofs_per_cell = fe.<a class="code" href="classFiniteElementData.html#ae2fa3b8d578ba488b4f37061bb0278bb">dofs_per_cell</a>;</div><div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>   n_q_points    = quadrature_formula.size();</div><div class="line"></div><div class="line">  <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a>   <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#aa57fdeca62a0708d77768a3bb2aeb826">cell_matrix</a> (dofs_per_cell, dofs_per_cell);</div><div class="line">  <a class="code" href="classVector.html">Vector&lt;double&gt;</a>       cell_rhs (dofs_per_cell);</div><div class="line"></div><div class="line">  std::vector&lt;types::global_dof_index&gt; local_dof_indices (dofs_per_cell);</div><div class="line"></div><div class="line">  std::vector&lt;double&gt;     lambda_values (n_q_points);</div><div class="line">  std::vector&lt;double&gt;     mu_values (n_q_points);</div><div class="line"></div><div class="line">  <a class="code" href="classConstantFunction.html">ConstantFunction&lt;dim&gt;</a> lambda(1.), <a class="code" href="namespacemu.html">mu</a>(1.);</div><div class="line"></div><div class="line">  RightHandSide&lt;dim&gt;      right_hand_side;</div><div class="line">  std::vector&lt;Vector&lt;double&gt; &gt; rhs_values (n_q_points,</div><div class="line">                                           <a class="code" href="classVector.html">Vector&lt;double&gt;</a>(dim));</div></div><!-- fragment --><p>The next thing is the loop over all elements. Note that we do not have to do <em>all</em> the work on every process: our job here is only to assemble the system on cells that actually belong to this MPI process, all other cells will be taken care of by other processes. This is what the if-clause immediately after the for-loop takes care of: it queries the subdomain identifier of each cell, which is a number associated with each cell that tells us which process owns it. In more generality, the subdomain id is used to split a domain into several parts (we do this above, at the beginning of <code>setup_system()</code>), and which allows to identify which subdomain a cell is living on. In this application, we have each process handle exactly one subdomain, so we identify the terms <code>subdomain</code> and <code>MPI process</code>.</p>
<p>Apart from this, assembling the local system is relatively uneventful if you have understood how this is done in <a class="el" href="step_8.html">step-8</a>. As mentioned above, distributing local contributions into the global matrix and right hand sides also takes care of hanging node constraints in the same way as is done in <a class="el" href="step_6.html">step-6</a>.</p>
<div class="fragment"><div class="line"><span class="keyword">typename</span> <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;::active_cell_iterator</a></div><div class="line">cell = dof_handler.<a class="code" href="classDoFHandler.html#a1a36dbbb4c54a7038c60ee9c8eab369a">begin_active</a>(),</div><div class="line">endc = dof_handler.<a class="code" href="classDoFHandler.html#a7b510a66ee9ea25720f64220496126ec">end</a>();</div><div class="line"><span class="keywordflow">for</span> (; cell!=endc; ++cell)</div><div class="line">  <span class="keywordflow">if</span> (cell-&gt;subdomain_id() == <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>)</div><div class="line">    {</div><div class="line">      <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#aa57fdeca62a0708d77768a3bb2aeb826">cell_matrix</a> = 0;</div><div class="line">      cell_rhs = 0;</div><div class="line"></div><div class="line">      fe_values.reinit (cell);</div><div class="line"></div><div class="line">      lambda.value_list (fe_values.get_quadrature_points(), lambda_values);</div><div class="line">      <a class="code" href="namespacemu.html">mu</a>.value_list     (fe_values.get_quadrature_points(), mu_values);</div><div class="line"></div><div class="line">      <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i=0; i&lt;dofs_per_cell; ++i)</div><div class="line">        {</div><div class="line">          <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span></div><div class="line">          component_i = fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j=0; j&lt;dofs_per_cell; ++j)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span></div><div class="line">              component_j = fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(j).first;</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point=0; q_point&lt;n_q_points;</div><div class="line">                   ++q_point)</div><div class="line">                {</div><div class="line">                  <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#aa57fdeca62a0708d77768a3bb2aeb826">cell_matrix</a>(i,j)</div><div class="line">                  +=</div><div class="line">                    (</div><div class="line">                      (fe_values.shape_grad(i,q_point)[component_i] *</div><div class="line">                       fe_values.shape_grad(j,q_point)[component_j] *</div><div class="line">                       lambda_values[q_point])</div><div class="line">                      +</div><div class="line">                      (fe_values.shape_grad(i,q_point)[component_j] *</div><div class="line">                       fe_values.shape_grad(j,q_point)[component_i] *</div><div class="line">                       mu_values[q_point])</div><div class="line">                      +</div><div class="line">                      ((component_i == component_j) ?</div><div class="line">                       (fe_values.shape_grad(i,q_point) *</div><div class="line">                        fe_values.shape_grad(j,q_point) *</div><div class="line">                        mu_values[q_point])  :</div><div class="line">                       0)</div><div class="line">                    )</div><div class="line">                    *</div><div class="line">                    fe_values.JxW(q_point);</div><div class="line">                }</div><div class="line">            }</div><div class="line">        }</div><div class="line"></div><div class="line">      right_hand_side.vector_value_list (fe_values.get_quadrature_points(),</div><div class="line">                                         rhs_values);</div><div class="line">      <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i=0; i&lt;dofs_per_cell; ++i)</div><div class="line">        {</div><div class="line">          <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span></div><div class="line">          component_i = fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point=0; q_point&lt;n_q_points; ++q_point)</div><div class="line">            cell_rhs(i) += fe_values.shape_value(i,q_point) *</div><div class="line">                           rhs_values[q_point](component_i) *</div><div class="line">                           fe_values.JxW(q_point);</div><div class="line">        }</div><div class="line"></div><div class="line">      cell-&gt;get_dof_indices (local_dof_indices);</div><div class="line">      hanging_node_constraints</div><div class="line">      .distribute_local_to_global(cell_matrix, cell_rhs,</div><div class="line">                                  local_dof_indices,</div><div class="line">                                  system_matrix, system_rhs);</div><div class="line">    }</div></div><!-- fragment --><p>The next step is to "compress" the vector and the system matrix. This means that each process sends the additions that were made above to those entries of the matrix and vector that the process did not own itself, to the process that owns them. After receiving these additions from other processes, each process then adds them to the values it already has.</p>
<div class="fragment"><div class="line">system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div></div><!-- fragment --><p>The global matrix and right hand side vectors have now been formed. We still have to apply boundary values, in the same way as we did, for example, in <a class="el" href="step_3.html">step-3</a>, <a class="el" href="step_4.html">step-4</a>, and a number of other programs.</p>
<p>The last argument to the call to <a class="el" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values()</a> below allows for some optimizations. It controls whether we should also delete entries (i.e., set them to zero) in the matrix columns corresponding to boundary nodes, or to keep them (and passing <code>true</code> means: yes, do eliminate the columns). If we do eliminate columns, then the resulting matrix will be symmetric again if it was before; if we don't, then it won't. The solution of the resulting system should be the same, though. The only reason why we may want to make the system symmetric again is that we would like to use the CG method, which only works with symmetric matrices. The reason why we may <em>not</em> want to make the matrix symmetric is because this would require us to write into column entries that actually reside on other processes, i.e., it involves communicating data. This is always expensive.</p>
<p>Experience tells us that CG also works (and works almost as well) if we don't remove the columns associated with boundary nodes, which can be explained by the special structure of this particular non-symmetry. To avoid the expense of communication, we therefore do not eliminate the entries in the affected columns.</p>
<div class="fragment"><div class="line">  std::map&lt;types::global_dof_index,double&gt; boundary_values;</div><div class="line">  <a class="code" href="namespaceVectorTools.html#a187aeb575be07bc47cb3dea1a47aaf88">VectorTools::interpolate_boundary_values</a> (dof_handler,</div><div class="line">                                            0,</div><div class="line">                                            <a class="code" href="classZeroFunction.html">ZeroFunction&lt;dim&gt;</a>(dim),</div><div class="line">                                            boundary_values);</div><div class="line">  <a class="code" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values</a> (boundary_values,</div><div class="line">                                      system_matrix,</div><div class="line">                                      solution,</div><div class="line">                                      system_rhs,</div><div class="line">                                      <span class="keyword">false</span>);</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemsolve"></a> </p><h4>ElasticProblem::solve</h4>
<p>Having assembled the linear system, we next need to solve it. PETSc offers a variety of sequential and parallel solvers, for which we have written wrappers that have almost the same interface as is used for the deal.II solvers used in all previous example programs. The following code should therefore look rather familiar.</p>
<p>At the top of the function, we set up a convergence monitor, and assign it the accuracy to which we would like to solve the linear system. Next, we create an actual solver object using PETSc's CG solver which also works with parallel (distributed) vectors and matrices. And finally a preconditioner; we choose to use a block Jacobi preconditioner which works by computing an incomplete LU decomposition on each diagonal block of the matrix. (In other words, each MPI process computes an ILU from the rows it stores by throwing away columns that correspond to row indices not stored locally; this yields a square matrix block from which we can compute an ILU. That means that if you run the program with only one process, then you will use an ILU(0) as a preconditioner, while if it is run on many processes, then we will have a number of blocks on the diagonal and the preconditioner is the ILU(0) of each of these blocks. In the extreme case of one degree of freedom per processor, this preconditioner is then simply a Jacobi preconditioner since the diagonal matrix blocks consist of only a single entry. Such a preconditioner is relatively easy to compute because it does not require any kind of communication between processors, but it is in general not very efficient for large numbers of processors.)</p>
<p>Following this kind of setup, we then solve the linear system:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> ElasticProblem&lt;dim&gt;::solve ()</div><div class="line">{</div><div class="line">  <a class="code" href="classSolverControl.html">SolverControl</a>           solver_control (solution.<a class="code" href="classVector.html#a8005bf1ec399c608c3755c1d22960add">size</a>(),</div><div class="line">                                          1<a class="code" href="namespacePhysics_1_1Elasticity_1_1Kinematics.html#a9587d5229555daa5b1fa1ba2f8a40adb">e</a>-8*system_rhs.l2_norm());</div><div class="line">  <a class="code" href="classPETScWrappers_1_1SolverCG.html">PETScWrappers::SolverCG</a> cg (solver_control,</div><div class="line">                              mpi_communicator);</div><div class="line"></div><div class="line">  <a class="code" href="classPETScWrappers_1_1PreconditionBlockJacobi.html">PETScWrappers::PreconditionBlockJacobi</a> preconditioner(system_matrix);</div><div class="line"></div><div class="line">  cg.solve (system_matrix, solution, system_rhs,</div><div class="line">            preconditioner);</div></div><!-- fragment --><p>The next step is to distribute hanging node constraints. This is a little tricky, since to fill in the value of a constrained node you need access to the values of the nodes to which it is constrained (for example, for a Q1 element in 2d, we need access to the two nodes on the big side of a hanging node face, to compute the value of the constrained node in the middle).</p>
<p>The problem is that we have built our vectors (in <code>setup_system()</code>) in such a way that every process is responsible for storing only those elements of the solution vector that correspond to the degrees of freedom this process "owns". There are, however, cases where in order to compute the value of the vector entry for a constrained degree of freedom on one process, we need to access vector entries that are stored on other processes. PETSc (and, for that matter, the MPI model on which it is built) does not allow to simply query vector entries stored on other processes, so what we do here is to get a copy of the "distributed" vector where we store all elements locally. This is simple, since the deal.II wrappers have a conversion constructor for the deal.II <a class="el" href="classVector.html">Vector</a> class. (This conversion of course requires communication, but in essence every process only needs to send its data to every other process once in bulk, rather than having to respond to queries for individual elements):</p>
<div class="fragment"><div class="line"><a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution (solution);</div></div><!-- fragment --><p>Of course, as in previous discussions, it is clear that such a step cannot scale very far if you wanted to solve large problems on large numbers of processes, because every process now stores <em>all elements</em> of the solution vector. (We will show how to do this better in <a class="el" href="step_40.html">step-40</a>.) On the other hand, distributing hanging node constraints is simple on this local copy, using the usual function ConstraintMatrix::distributed(). In particular, we can compute the values of <em>all</em> constrained degrees of freedom, whether the current process owns them or not:</p>
<div class="fragment"><div class="line">hanging_node_constraints.distribute (localized_solution);</div></div><!-- fragment --><p>Then transfer everything back into the global vector. The following operation copies those elements of the localized solution that we store locally in the distributed solution, and does not touch the other ones. Since we do the same operation on all processors, we end up with a distributed vector (i.e., a vector that on every process only stores the vector entries corresponding to degrees of freedom that are owned by this process) that has all the constrained nodes fixed.</p>
<p>We end the function by returning the number of iterations it took to converge, to allow for some output.</p>
<div class="fragment"><div class="line">  solution = localized_solution;</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> solver_control.last_step();</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemrefine_grid"></a> </p><h4>ElasticProblem::refine_grid</h4>
<p>Using some kind of refinement indicator, the mesh can be refined. The problem is basically the same as with distributing hanging node constraints: in order to compute the error indicator (even if we were just interested in the indicator on the cells the current process owns), we need access to more elements of the solution vector than just those the current processor stores. To make this happen, we do essentially what we did in <code>solve()</code> already, namely get a <em>complete</em> copy of the solution vector onto every process, and use that to compute. This is, in itself expensive as explained above, and it is particular unnecessary since we had just created and then destroyed such a vector in <code>solve()</code>, but efficiency is not the point of this program and so let us opt for a design in which every function is as self-contained as possible.</p>
<p>Once we have such a "localized" vector that contains <em>all</em> elements of the solution vector, we can compute the indicators for the cells that belong to the present process. In fact, we could of course compute <em>all</em> refinement indicators since our <a class="el" href="classTriangulation.html">Triangulation</a> and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects store information about all cells, and since we have a complete copy of the solution vector. But in the interest in showing how to operate in parallel, let us demonstrate how one would operate if one were to only compute <em>some</em> error indicators and then exchange the remaining ones with the other processes. (Ultimately, each process needs a complete set of refinement indicators because every process needs to refine their mesh, and needs to refine it in exactly the same way as all of the other processes.)</p>
<p>So, to do all of this, we need to:</p><ul>
<li>First, get a local copy of the distributed solution vector.</li>
<li>Second, create a vector to store the refinement indicators.</li>
<li>Third, let the <a class="el" href="classKellyErrorEstimator.html">KellyErrorEstimator</a> compute refinement indicators for all cells belonging to the present subdomain/process. The last argument of the call indicates which subdomain we are interested in. The three arguments before it are various other default arguments that one usually doesn't need (and doesn't state values for, but rather uses the defaults), but which we have to state here explicitly since we want to modify the value of a following argument (i.e. the one indicating the subdomain).</li>
</ul>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::refine_grid ()</div><div class="line">{</div><div class="line">  <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution (solution);</div><div class="line"></div><div class="line">  <a class="code" href="classVector.html">Vector&lt;float&gt;</a> local_error_per_cell (triangulation.n_active_cells());</div><div class="line">  <a class="code" href="classKellyErrorEstimator.html#a971b0bfe57fa21867ed3c06794487e4b">KellyErrorEstimator&lt;dim&gt;::estimate</a> (dof_handler,</div><div class="line">                                      <a class="code" href="classQGauss.html">QGauss&lt;dim-1&gt;</a>(2),</div><div class="line">                                      <span class="keyword">typename</span> <a class="code" href="structFunctionMap.html#a6bb95bc991dd3337330f1c725f59b008">FunctionMap&lt;dim&gt;::type</a>(),</div><div class="line">                                      localized_solution,</div><div class="line">                                      local_error_per_cell,</div><div class="line">                                      <a class="code" href="classComponentMask.html">ComponentMask</a>(),</div><div class="line">                                      <span class="keyword">nullptr</span>,</div><div class="line">                                      <a class="code" href="classMultithreadInfo.html#ad0b84ae105b385b88bdd4bfc0c530995">MultithreadInfo::n_threads</a>(),</div><div class="line">                                      this_mpi_process);</div></div><!-- fragment --><p>Now all processes have computed error indicators for their own cells and stored them in the respective elements of the <code>local_error_per_cell</code> vector. The elements of this vector for cells not owned by the present process are zero. However, since all processes have a copy of the entire triangulation and need to keep these copies in sync, they need the values of refinement indicators for all cells of the triangulation. Thus, we need to distribute our results. We do this by creating a distributed vector where each process has its share, and sets the elements it has computed. Consequently, when you view this vector as one that lives across all processes, then every element of this vector has been set once. We can then assign this parallel vector to a local, non-parallel vector on each process, making <em>all</em> error indicators available on every process.</p>
<p>So in the first step, we need to set up a parallel vector. For simplicity, every process will own a chunk with as many elements as this process owns cells, so that the first chunk of elements is stored with process zero, the next chunk with process one, and so on. It is important to remark, however, that these elements are not necessarily the ones we will write to. This is so, since the order in which cells are arranged, i.e., the order in which the elements of the vector correspond to cells, is not ordered according to the subdomain these cells belong to. In other words, if on this process we compute indicators for cells of a certain subdomain, we may write the results to more or less random elements of the distributed vector; in particular, they may not necessarily lie within the chunk of vector we own on the present process. They will subsequently have to be copied into another process's memory space, an operation that PETSc does for us when we call the <code>compress()</code> function. This inefficiency could be avoided with some more code, but we refrain from it since it is not a major factor in the program's total runtime.</p>
<p>So here's how we do it: count how many cells belong to this process, set up a distributed vector with that many elements to be stored locally, and copy over the elements we computed locally, then compress the result. In fact, we really only copy the elements that are nonzero, so we may miss a few that we computed to zero, but this won't hurt since the original values of the vector is zero anyway.</p>
<div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_local_cells</div><div class="line">  = <a class="code" href="namespaceGridTools.html#a8c212a30784bec20b1ae13fad3fd579c">GridTools::count_cells_with_subdomain_association</a> (triangulation,</div><div class="line">                                                       this_mpi_process);</div><div class="line"><a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a></div><div class="line">distributed_all_errors (mpi_communicator,</div><div class="line">                        triangulation.n_active_cells(),</div><div class="line">                        n_local_cells);</div><div class="line"></div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i=0; i&lt;local_error_per_cell.size(); ++i)</div><div class="line">  <span class="keywordflow">if</span> (local_error_per_cell(i) != 0)</div><div class="line">    distributed_all_errors(i) = local_error_per_cell(i);</div><div class="line">distributed_all_errors.compress (<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae5042eefddc828c7c31e1e8e26da8b09">VectorOperation::insert</a>);</div></div><!-- fragment --><p>So now we have this distributed vector that contains the refinement indicators for all cells. To use it, we need to obtain a local copy and then use it to mark cells for refinement or coarsening, and actually do the refinement and coarsening. It is important to recognize that <em>every</em> process does this to its own copy of the triangulation, and does it in exactly the same way.</p>
<div class="fragment"><div class="line">  <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;float&gt;</a> localized_all_errors (distributed_all_errors);</div><div class="line"></div><div class="line">  <a class="code" href="namespaceGridRefinement.html#a2500638aae40fe3bfbf094754645dc57">GridRefinement::refine_and_coarsen_fixed_number</a> (triangulation,</div><div class="line">                                                   localized_all_errors,</div><div class="line">                                                   0.3, 0.03);</div><div class="line">  triangulation.execute_coarsening_and_refinement ();</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemoutput_results"></a> </p><h4>ElasticProblem::output_results</h4>
<p>The final function of significant interest is the one that creates graphical output. This works the same way as in <a class="el" href="step_8.html">step-8</a>, with two small differences. Before discussing these, let us state the general philosophy this function will work: we intend for all of the data to be generated on a single process, and subsequently written to a file. This is, as many other parts of this program already discussed, not something that will scale. Previously, we had argued that we will get into trouble with triangulations, DoFHandlers, and copies of the solution vector where every process has to store all of the data, and that there will come to be a point where each process simply doesn't have enough memory to store that much data. Here, the situation is different: it's not only the memory, but also the run time that's a problem. If one process is responsible for processing <em>all</em> of the data while all of the other processes do nothing, then this one function will eventually come to dominate the overall run time of the program. In particular, the time this function takes is going to be proportional to the overall size of the problem (counted in the number of cells, or the number of degrees of freedom), independent of the number of processes we throw at it.</p>
<p>Such situations need to be avoided, and we will show in <a class="el" href="step_18.html">step-18</a> and <a class="el" href="step_40.html">step-40</a> how to address this issue. For the current problem, the solution is to have each process generate output data only for its own local cells, and write them to separate files, one file per process. This is how <a class="el" href="step_18.html">step-18</a> operates. Alternatively, one could simply leave everything in a set of independent files and let the visualization software read all of them (possibly also using multiple processors) and create a single visualization out of all of them; this is the path <a class="el" href="step_40.html">step-40</a>, <a class="el" href="step_32.html">step-32</a>, and all other parallel programs developed later on take.</p>
<p>More specifically for the current function, all processes call this function, but not all of them need to do the work associated with generating output. In fact, they shouldn't, since we would try to write to the same file multiple times at once. So we let only the first process do this, and all the other ones idle around during this time (or start their work for the next iteration, or simply yield their CPUs to other jobs that happen to run at the same time). The second thing is that we not only output the solution vector, but also a vector that indicates which subdomain each cell belongs to. This will make for some nice pictures of partitioned domains.</p>
<p>To implement this, process zero needs a complete set of solution components in a local vector. Just as with the previous function, the efficient way to do this would be to re-use the vector already created in the <code>solve()</code> function, but to keep things more self-contained, we simply re-create one here from the distributed solution vector.</p>
<p>An important thing to realize is that we do this localization operation on all processes, not only the one that actually needs the data. This can't be avoided, however, with the communication model of MPI: MPI does not have a way to query data on another process, both sides have to initiate a communication at the same time. So even though most of the processes do not need the localized solution, we have to place the statement converting the distributed into a localized vector so that all processes execute it.</p>
<p>(Part of this work could in fact be avoided. What we do is send the local parts of all processes to all other processes. What we would really need to do is to initiate an operation on all processes where each process simply sends its local chunk of data to process zero, since this is the only one that actually needs it, i.e., we need something like a gather operation. PETSc can do this, but for simplicity's sake we don't attempt to make use of this here. We don't, since what we do is not very expensive in the grand scheme of things: it is one vector communication among all processes , which has to be compared to the number of communications we have to do when solving the linear system, setting up the block-ILU for the preconditioner, and other operations.)</p>
<div class="fragment"><div class="line"><span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line"><span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::output_results (<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword"></span>{</div><div class="line">  <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution (solution);</div></div><!-- fragment --><p>This being done, process zero goes ahead with setting up the output file as in <a class="el" href="step_8.html">step-8</a>, and attaching the (localized) solution vector to the output object. (The code to generate the output file name is stolen and slightly modified from <a class="el" href="step_5.html">step-5</a>, since we expect that we can do a number of cycles greater than 10, which is the maximum of what the code in <a class="el" href="step_8.html">step-8</a> could handle.)</p>
<div class="fragment"><div class="line"><span class="keywordflow">if</span> (this_mpi_process == 0)</div><div class="line">  {</div><div class="line">    std::ostringstream filename;</div><div class="line">    filename &lt;&lt; <span class="stringliteral">&quot;solution-&quot;</span> &lt;&lt; cycle &lt;&lt; <span class="stringliteral">&quot;.vtk&quot;</span>;</div><div class="line"></div><div class="line">    std::ofstream output (filename.str().c_str());</div><div class="line"></div><div class="line">    <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">    data_out.<a class="code" href="classDataOut__DoFData.html#ac1eb26168177faa30ffbcf9cbb9c3cd5">attach_dof_handler</a> (dof_handler);</div><div class="line"></div><div class="line">    std::vector&lt;std::string&gt; solution_names;</div><div class="line">    <span class="keywordflow">switch</span> (dim)</div><div class="line">      {</div><div class="line">      <span class="keywordflow">case</span> 1:</div><div class="line">        solution_names.push_back (<span class="stringliteral">&quot;displacement&quot;</span>);</div><div class="line">        <span class="keywordflow">break</span>;</div><div class="line">      <span class="keywordflow">case</span> 2:</div><div class="line">        solution_names.push_back (<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">        solution_names.push_back (<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">        <span class="keywordflow">break</span>;</div><div class="line">      <span class="keywordflow">case</span> 3:</div><div class="line">        solution_names.push_back (<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">        solution_names.push_back (<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">        solution_names.push_back (<span class="stringliteral">&quot;z_displacement&quot;</span>);</div><div class="line">        <span class="keywordflow">break</span>;</div><div class="line">      <span class="keywordflow">default</span>:</div><div class="line">        <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a> (<span class="keyword">false</span>, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line">      }</div><div class="line"></div><div class="line">    data_out.add_data_vector (localized_solution, solution_names);</div></div><!-- fragment --><p>The only other thing we do here is that we also output one value per cell indicating which subdomain (i.e., MPI process) it belongs to. This requires some conversion work, since the data the library provides us with is not the one the output class expects, but this is not difficult. First, set up a vector of integers, one per cell, that is then filled by the subdomain id of each cell.</p>
<p>The elements of this vector are then converted to a floating point vector in a second step, and this vector is added to the <a class="el" href="classDataOut.html">DataOut</a> object, which then goes off creating output in VTK format:</p>
<div class="fragment"><div class="line">      std::vector&lt;unsigned int&gt; partition_int (triangulation.n_active_cells());</div><div class="line">      <a class="code" href="namespaceGridTools.html#ae0cb61bdd7e17c6b1589bdd16891e561">GridTools::get_subdomain_association</a> (triangulation, partition_int);</div><div class="line"></div><div class="line">      <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> partitioning(partition_int.begin(),</div><div class="line">                                        partition_int.end());</div><div class="line"></div><div class="line">      data_out.add_data_vector (partitioning, <span class="stringliteral">&quot;partitioning&quot;</span>);</div><div class="line"></div><div class="line">      data_out.build_patches ();</div><div class="line">      data_out.write_vtk (output);</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="ElasticProblemrun"></a> </p><h4>ElasticProblem::run</h4>
<p>Lastly, here is the driver function. It is almost completely unchanged from <a class="el" href="step_8.html">step-8</a>, with the exception that we replace <code>std::cout</code> by the <code>pcout</code> stream. Apart from this, the only other cosmetic change is that we output how many degrees of freedom there are per process, and how many iterations it took for the linear solver to converge:</p>
<div class="fragment"><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::run ()</div><div class="line">  {</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle=0; cycle&lt;10; ++cycle)</div><div class="line">      {</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">          {</div><div class="line">            <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a> (triangulation, -1, 1);</div><div class="line">            triangulation.refine_global (3);</div><div class="line">          }</div><div class="line">        <span class="keywordflow">else</span></div><div class="line">          refine_grid ();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">              &lt;&lt; triangulation.n_active_cells()</div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        setup_system ();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span></div><div class="line">              &lt;&lt; dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>()</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot; (by partition:&quot;</span>;</div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p=0; p&lt;<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>; ++p)</div><div class="line">          pcout &lt;&lt; (p==0 ? <span class="charliteral">&#39; &#39;</span> : <span class="charliteral">&#39;+&#39;</span>)</div><div class="line">                &lt;&lt; (<a class="code" href="namespaceDoFTools.html#a2dde9ded6d7cbc9962547356f37e5f72">DoFTools::</a></div><div class="line"><a class="code" href="namespaceDoFTools.html#a2dde9ded6d7cbc9962547356f37e5f72">                    count_dofs_with_subdomain_association</a> (dof_handler,</div><div class="line">                                                           p));</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;)&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        assemble_system ();</div><div class="line">        <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_iterations = solve ();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Solver converged in &quot;</span> &lt;&lt; n_iterations</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        output_results (cycle);</div><div class="line">      }</div><div class="line">  }</div><div class="line">}</div></div><!-- fragment --><p><a class="anchor" id="Thecodemaincodefunction"></a> </p><h3>The <code>main</code> function</h3>
<p>The <code>main()</code> works the same way as most of the main functions in the other example programs, i.e., it delegates work to the <code>run</code> function of a master object, and only wraps everything into some code to catch exceptions:</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> main (<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> **argv)</div><div class="line">{</div><div class="line">  <span class="keywordflow">try</span></div><div class="line">    {</div><div class="line">      <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">      <span class="keyword">using namespace </span>Step17;</div></div><!-- fragment --><p>Here is the only real difference: MPI and PETSc both require that we initialize these libraries at the beginning of the program, and un-initialize them at the end. The class MPI_InitFinalize takes care of all of that.</p>
<div class="fragment"><div class="line">      <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">      ElasticProblem&lt;2&gt; elastic_problem;</div><div class="line">      elastic_problem.run ();</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (...)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p> <a class="anchor" id="Results"></a></p><h1>Results</h1>
<p>If the program above is compiled and run on a single processor machine, it should generate results that are very similar to those that we already got with <a class="el" href="step_8.html">step-8</a>. However, it becomes more interesting if we run it on a multicore machine or a cluster of computers. The most basic way to run MPI programs is using a command line like </p><div class="fragment"><div class="line">mpirun -np 32 ./step-17</div></div><!-- fragment --><p> to run the <a class="el" href="step_17.html">step-17</a> executable with 32 processors.</p>
<p>(If you work on a cluster, then there is typically a step in between where you need to set up a job script and submit the script to a scheduler. The scheduler will execute the script whenever it can allocate 32 unused processors for your job. How to write such job scripts differs from cluster to cluster, and you should find the documentation of your cluster to see how to do this. On my system, I have to use the command <code>qsub</code> with a whole host of options to run a job in parallel.)</p>
<p>Whether directly or through a scheduler, if you run this program on 8 processors, you should get output like the following: </p><div class="fragment"><div class="line">Cycle 0:</div><div class="line">   Number of active cells:       64</div><div class="line">   Number of degrees of freedom: 162 (by <a class="code" href="namespaceSparsityTools.html#a4673bdd5ed26b4492008ac8366754f9d">partition</a>: 22+22+20+20+18+16+20+24)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 23 iterations.</div><div class="line">Cycle 1:</div><div class="line">   Number of active cells:       124</div><div class="line">   Number of degrees of freedom: 302 (by partition: 38+42+36+34+44+44+36+28)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 35 iterations.</div><div class="line">Cycle 2:</div><div class="line">   Number of active cells:       238</div><div class="line">   Number of degrees of freedom: 570 (by <a class="code" href="namespaceSparsityTools.html#a4673bdd5ed26b4492008ac8366754f9d">partition</a>: 68+80+66+74+58+68+78+78)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 46 iterations.</div><div class="line">Cycle 3:</div><div class="line">   Number of active cells:       454</div><div class="line">   Number of degrees of freedom: 1046 (by partition: 120+134+124+130+154+138+122+124)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 55 iterations.</div><div class="line">Cycle 4:</div><div class="line">   Number of active cells:       868</div><div class="line">   Number of degrees of freedom: 1926 (by <a class="code" href="namespaceSparsityTools.html#a4673bdd5ed26b4492008ac8366754f9d">partition</a>: 232+276+214+248+230+224+234+268)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 77 iterations.</div><div class="line">Cycle 5:</div><div class="line">   Number of active cells:       1654</div><div class="line">   Number of degrees of freedom: 3550 (by partition: 418+466+432+470+442+474+424+424)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 93 iterations.</div><div class="line">Cycle 6:</div><div class="line">   Number of active cells:       3136</div><div class="line">   Number of degrees of freedom: 6702 (by <a class="code" href="namespaceSparsityTools.html#a4673bdd5ed26b4492008ac8366754f9d">partition</a>: 838+796+828+892+866+798+878+806)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 127 iterations.</div><div class="line">Cycle 7:</div><div class="line">   Number of active cells:       5962</div><div class="line">   Number of degrees of freedom: 12446 (by partition: 1586+1484+1652+1552+1556+1576+1560+1480)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 158 iterations.</div><div class="line">Cycle 8:</div><div class="line">   Number of active cells:       11320</div><div class="line">   Number of degrees of freedom: 23586 (by <a class="code" href="namespaceSparsityTools.html#a4673bdd5ed26b4492008ac8366754f9d">partition</a>: 2988+2924+2890+2868+2864+3042+2932+3078)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 225 iterations.</div><div class="line">Cycle 9:</div><div class="line">   Number of active cells:       21424</div><div class="line">   Number of degrees of freedom: 43986 (by partition: 5470+5376+5642+5450+5630+5470+5416+5532)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 282 iterations.</div><div class="line">Cycle 10:</div><div class="line">   Number of active cells:       40696</div><div class="line">   Number of degrees of freedom: 83754 (by <a class="code" href="namespaceSparsityTools.html#a4673bdd5ed26b4492008ac8366754f9d">partition</a>: 10660+10606+10364+10258+10354+10322+10586+10604)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 392 iterations.</div><div class="line">Cycle 11:</div><div class="line">   Number of active cells:       76978</div><div class="line">   Number of degrees of freedom: 156490 (by partition: 19516+20148+19390+19390+19336+19450+19730+19530)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 509 iterations.</div><div class="line">Cycle 12:</div><div class="line">   Number of active cells:       146206</div><div class="line">   Number of degrees of freedom: 297994 (by <a class="code" href="namespaceSparsityTools.html#a4673bdd5ed26b4492008ac8366754f9d">partition</a>: 37462+37780+37000+37060+37232+37328+36860+37272)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 705 iterations.</div><div class="line">Cycle 13:</div><div class="line">   Number of active cells:       276184</div><div class="line">   Number of degrees of freedom: 558766 (by partition: 69206+69404+69882+71266+70348+69616+69796+69248)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 945 iterations.</div><div class="line">Cycle 14:</div><div class="line">   Number of active cells:       523000</div><div class="line">   Number of degrees of freedom: 1060258 (by <a class="code" href="namespaceSparsityTools.html#a4673bdd5ed26b4492008ac8366754f9d">partition</a>: 132928+132296+131626+132172+132170+133588+132252+133226)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 1282 iterations.</div><div class="line">Cycle 15:</div><div class="line">   Number of active cells:       987394</div><div class="line">   Number of degrees of freedom: 1994226 (by partition: 253276+249068+247430+248402+248496+251380+248272+247902)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 1760 iterations.</div><div class="line">Cycle 16:</div><div class="line">   Number of active cells:       1867477</div><div class="line">   Number of degrees of freedom: 3771884 (by <a class="code" href="namespaceSparsityTools.html#a4673bdd5ed26b4492008ac8366754f9d">partition</a>: 468452+474204+470818+470884+469960+</div><div class="line">471186+470686+475694)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 2251 iterations.</div></div><!-- fragment --><p> (This run uses a few more refinement cycles than the code available in the examples/ directory. The run also used a version of METIS from 2004 that generated different partitionings; consequently, the numbers you get today are slightly different.)</p>
<p>As can be seen, we can easily get to almost four million unknowns. In fact, the code's runtime with 8 processes was less than 7 minutes up to (and including) cycle 14, and 14 minutes including the second to last step. (These are numbers relevant to when the code was initially written, in 2004.) I lost the timing information for the last step, though, but you get the idea. All this is after release mode has been enabled by running <code>make release</code>, and with the generation of graphical output switched off for the reasons stated in the program comments above. (See also <a href="http://www.math.colostate.edu/~bangerth/videos.676.18.html">video lecture 18</a>.) The biggest 2d computations I did had roughly 7.1 million unknowns, and were done on 32 processes. It took about 40 minutes. Not surprisingly, the limiting factor for how far one can go is how much memory one has, since every process has to hold the entire mesh and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects, although matrices and vectors are split up. For the 7.1M computation, the memory consumption was about 600 bytes per unknown, which is not bad, but one has to consider that this is for every unknown, whether we store the matrix and vector entries locally or not.</p>
<p>Here is some output generated in the 12th cycle of the program, i.e. with roughly 300,000 unknowns:</p>
<table align="center" style="width:80%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-ux.png" width="100%"/>
</div>
 </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-uy.png" width="100%"/>
</div>
  </td></tr>
</table>
<p>As one would hope for, the x- (left) and y-displacements (right) shown here closely match what we already saw in <a class="el" href="step_8.html">step-8</a>. As shown there and in <a class="el" href="step_22.html">step-22</a>, we could as well have produced a vector plot of the displacement field, rather than plotting it as two separate scalar fields. What may be more interesting, though, is to look at the mesh and partition at this step:</p>
<table align="center" width="80%">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-grid.png" width="100%"/>
</div>
 </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.12-partition.png" width="100%"/>
</div>
  </td></tr>
</table>
<p>Again, the mesh (left) shows the same refinement pattern as seen previously. The right panel shows the partitioning of the domain across the 8 processes, each indicated by a different color. The picture shows that the subdomains are smaller where mesh cells are small, a fact that needs to be expected given that the partitioning algorithm tries to equilibrate the number of cells in each subdomain; this equilibration is also easily identified in the output shown above, where the number of degrees per subdomain is roughly the same.</p>
<p>It is worth noting that if we ran the same program with a different number of processes, that we would likely get slightly different output: a different mesh, different number of unknowns and iterations to convergence. The reason for this is that while the matrix and right hand side are the same independent of the number of processes used, the preconditioner is not: it performs an ILU(0) on the chunk of the matrix of <em>each processor separately</em>. Thus, it's effectiveness as a preconditioner diminishes as the number of processes increases, which makes the number of iterations increase. Since a different preconditioner leads to slight changes in the computed solution, this will then lead to slightly different mesh cells tagged for refinement, and larger differences in subsequent steps. The solution will always look very similar, though.</p>
<p>Finally, here are some results for a 3d simulation. You can repeat these by changing </p><div class="fragment"><div class="line">ElasticProblem&lt;2&gt; elastic_problem;</div></div><!-- fragment --><p> to </p><div class="fragment"><div class="line">ElasticProblem&lt;3&gt; elastic_problem;</div></div><!-- fragment --><p> in the main function. If you then run the program in parallel, you get something similar to this (this is for a job with 16 processes): </p><div class="fragment"><div class="line">Cycle 0:</div><div class="line">   Number of active cells:       512</div><div class="line">   Number of degrees of freedom: 2187 (by <a class="code" href="namespaceSparsityTools.html#a4673bdd5ed26b4492008ac8366754f9d">partition</a>: 114+156+150+114+114+210+105+102+120+120+96+123+141+183+156+183)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 27 iterations.</div><div class="line">Cycle 1:</div><div class="line">   Number of active cells:       1604</div><div class="line">   Number of degrees of freedom: 6549 (by partition: 393+291+342+354+414+417+570+366+444+288+543+525+345+387+489+381)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 42 iterations.</div><div class="line">Cycle 2:</div><div class="line">   Number of active cells:       4992</div><div class="line">   Number of degrees of freedom: 19167 (by <a class="code" href="namespaceSparsityTools.html#a4673bdd5ed26b4492008ac8366754f9d">partition</a>: 1428+1266+1095+1005+1455+1257+1410+1041+1320+1380+1080+1050+963+1005+1188+1224)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 65 iterations.</div><div class="line">Cycle 3:</div><div class="line">   Number of active cells:       15485</div><div class="line">   Number of degrees of freedom: 56760 (by partition: 3099+3714+3384+3147+4332+3858+3615+3117+3027+3888+3942+3276+4149+3519+3030+3663)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 96 iterations.</div><div class="line">Cycle 4:</div><div class="line">   Number of active cells:       48014</div><div class="line">   Number of degrees of freedom: 168762 (by <a class="code" href="namespaceSparsityTools.html#a4673bdd5ed26b4492008ac8366754f9d">partition</a>: 11043+10752+9846+10752+9918+10584+10545+11433+12393+11289+10488+9885+10056+9771+11031+8976)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 132 iterations.</div><div class="line">Cycle 5:</div><div class="line">   Number of active cells:       148828</div><div class="line">   Number of degrees of freedom: 492303 (by partition: 31359+30588+34638+32244+30984+28902+33297+31569+29778+29694+28482+28032+32283+30702+31491+28260)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 179 iterations.</div><div class="line">Cycle 6:</div><div class="line">   Number of active cells:       461392</div><div class="line">   Number of degrees of freedom: 1497951 (by <a class="code" href="namespaceSparsityTools.html#a4673bdd5ed26b4492008ac8366754f9d">partition</a>: 103587+100827+97611+93726+93429+88074+95892+88296+96882+93000+87864+90915+92232+86931+98091+90594)</div><div class="line">   <a class="code" href="classSolver.html">Solver</a> converged in 261 iterations.</div></div><!-- fragment --><p>The last step, going up to 1.5 million unknowns, takes about 55 minutes with 16 processes on 8 dual-processor machines (of the kind available in 2003). The graphical output generated by this job is rather large (cycle 5 already prints around 82 MB of data), so we contend ourselves with showing output from cycle 4:</p>
<table width="80%" align="center">
<tr>
<td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.4-3d-partition.png" width="100%"/>
</div>
 </td><td><div class="image">
<img src="https://www.dealii.org/images/steps/developer/step-17.4-3d-ux.png" width="100%"/>
</div>
  </td></tr>
</table>
<p>The left picture shows the partitioning of the cube into 16 processes, whereas the right one shows the x-displacement along two cutplanes through the cube.</p>
<p><a class="anchor" id="extensions"></a> <a class="anchor" id="Possibilitiesforextensions"></a></p><h3>Possibilities for extensions</h3>
<p>The program keeps a complete copy of the <a class="el" href="classTriangulation.html">Triangulation</a> and <a class="el" href="classDoFHandler.html">DoFHandler</a> objects on every processor. It also creates complete copies of the solution vector, and it creates output on only one processor. All of this is obviously the bottleneck as far as parallelization is concerned.</p>
<p>Internally, within deal.II, parallelizing the data structures used in hierarchic and unstructured triangulations is a hard problem, and it took us a few more years to make this happen. The <a class="el" href="step_40.html">step-40</a> tutorial program and the <a class="el" href="group__distributed.html">Parallel computing with multiple processors using distributed memory</a> documentation module talk about how to do these steps and what it takes from an application perspective. An obvious extension of the current program would be to use this functionality to completely distribute computations to many more processors than used here. <a class="anchor" id="PlainProg"></a> </p><h1>The plain program</h1>
<div class="fragment"><div class="line"><span class="comment">/* ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Copyright (C) 2000 - 2016 by the deal.II authors</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * This file is part of the deal.II library.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * The deal.II library is free software; you can use it, redistribute</span></div><div class="line"><span class="comment"> * it, and/or modify it under the terms of the GNU Lesser General</span></div><div class="line"><span class="comment"> * Public License as published by the Free Software Foundation; either</span></div><div class="line"><span class="comment"> * version 2.1 of the License, or (at your option) any later version.</span></div><div class="line"><span class="comment"> * The full text of the license can be found in the file LICENSE at</span></div><div class="line"><span class="comment"> * the top level of the deal.II distribution.</span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * ---------------------------------------------------------------------</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment"> *</span></div><div class="line"><span class="comment"> * Author: Wolfgang Bangerth, University of Texas at Austin, 2000, 2004</span></div><div class="line"><span class="comment"> *         Wolfgang Bangerth, Texas A&amp;M University, 2016</span></div><div class="line"><span class="comment"> */</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;deal.II/base/quadrature_lib.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/base/function.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/base/logstream.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/base/multithread_info.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/vector.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/full_matrix.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/constraint_matrix.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/dynamic_sparsity_pattern.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/sparsity_tools.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/tria.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/grid_generator.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/grid_refinement.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/tria_accessor.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/tria_iterator.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/tria_boundary_lib.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/dofs/dof_handler.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/dofs/dof_accessor.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/dofs/dof_tools.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/fe/fe_values.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/fe/fe_system.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/fe/fe_q.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/numerics/vector_tools.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/numerics/matrix_tools.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/numerics/data_out.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/numerics/error_estimator.h&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;deal.II/base/conditional_ostream.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/base/mpi.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/petsc_parallel_vector.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/petsc_parallel_sparse_matrix.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/petsc_solver.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/lac/petsc_precondition.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/grid/grid_tools.h&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;deal.II/dofs/dof_renumbering.h&gt;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;sstream&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">namespace </span>Step17</div><div class="line">{</div><div class="line">  <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keyword">class </span>ElasticProblem</div><div class="line">  {</div><div class="line">  <span class="keyword">public</span>:</div><div class="line">    ElasticProblem ();</div><div class="line">    ~ElasticProblem ();</div><div class="line">    <span class="keywordtype">void</span> <a class="code" href="namespaceWorkStream.html#a0c5d332d74a4df80784140218896b169">run</a> ();</div><div class="line"></div><div class="line">  <span class="keyword">private</span>:</div><div class="line">    <span class="keywordtype">void</span> setup_system ();</div><div class="line">    <span class="keywordtype">void</span> assemble_system ();</div><div class="line">    <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> solve ();</div><div class="line">    <span class="keywordtype">void</span> refine_grid ();</div><div class="line">    <span class="keywordtype">void</span> output_results (<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle) <span class="keyword">const</span>;</div><div class="line"></div><div class="line">    MPI_Comm mpi_communicator;</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>;</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>;</div><div class="line"></div><div class="line">    <a class="code" href="classConditionalOStream.html">ConditionalOStream</a> pcout;</div><div class="line"></div><div class="line">    <a class="code" href="classTriangulation.html">Triangulation&lt;dim&gt;</a>   triangulation;</div><div class="line">    <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;</a>      dof_handler;</div><div class="line"></div><div class="line">    <a class="code" href="classFESystem.html">FESystem&lt;dim&gt;</a>        fe;</div><div class="line"></div><div class="line">    <a class="code" href="classConstraintMatrix.html">ConstraintMatrix</a>     hanging_node_constraints;</div><div class="line"></div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1SparseMatrix.html">PETScWrappers::MPI::SparseMatrix</a> system_matrix;</div><div class="line"></div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a>       solution;</div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a>       system_rhs;</div><div class="line">  };</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keyword">class </span>RightHandSide :  <span class="keyword">public</span> <a class="code" href="classFunction.html">Function</a>&lt;dim&gt;</div><div class="line">  {</div><div class="line">  <span class="keyword">public</span>:</div><div class="line">    RightHandSide ();</div><div class="line"></div><div class="line">    <span class="keyword">virtual</span> <span class="keywordtype">void</span> vector_value (<span class="keyword">const</span> <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &amp;p,</div><div class="line">                               <a class="code" href="classVector.html">Vector&lt;double&gt;</a>   &amp;values) <span class="keyword">const</span>;</div><div class="line"></div><div class="line">    <span class="keyword">virtual</span> <span class="keywordtype">void</span> vector_value_list (<span class="keyword">const</span> std::vector&lt;<a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &gt; &amp;points,</div><div class="line">                                    std::vector&lt;<a class="code" href="classVector.html">Vector&lt;double&gt;</a> &gt;   &amp;value_list) <span class="keyword">const</span>;</div><div class="line">  };</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  RightHandSide&lt;dim&gt;::RightHandSide () :</div><div class="line">    <a class="code" href="classFunction.html">Function</a>&lt;dim&gt; (dim)</div><div class="line">  {}</div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keyword">inline</span></div><div class="line">  <span class="keywordtype">void</span> RightHandSide&lt;dim&gt;::vector_value (<span class="keyword">const</span> <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &amp;p,</div><div class="line">                                         <a class="code" href="classVector.html">Vector&lt;double&gt;</a>   &amp;values)<span class="keyword"> const</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a> (values.<a class="code" href="classVector.html#a8005bf1ec399c608c3755c1d22960add">size</a>() == dim,</div><div class="line">            <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a> (values.<a class="code" href="classVector.html#a8005bf1ec399c608c3755c1d22960add">size</a>(), dim));</div><div class="line">    <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a> (dim &gt;= 2, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line"></div><div class="line">    <a class="code" href="classPoint.html">Point&lt;dim&gt;</a> point_1, point_2;</div><div class="line">    point_1(0) = 0.5;</div><div class="line">    point_2(0) = -0.5;</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (((p-point_1).norm_square() &lt; 0.2*0.2) ||</div><div class="line">        ((p-point_2).norm_square() &lt; 0.2*0.2))</div><div class="line">      values(0) = 1;</div><div class="line">    <span class="keywordflow">else</span></div><div class="line">      values(0) = 0;</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (p.<a class="code" href="classPoint.html#a859ea7f3bf3e64be2e0f5ed1bfcc8550">square</a>() &lt; 0.2*0.2)</div><div class="line">      values(1) = 1;</div><div class="line">    <span class="keywordflow">else</span></div><div class="line">      values(1) = 0;</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> RightHandSide&lt;dim&gt;::vector_value_list (<span class="keyword">const</span> std::vector&lt;<a class="code" href="classPoint.html">Point&lt;dim&gt;</a> &gt; &amp;points,</div><div class="line">                                              std::vector&lt;<a class="code" href="classVector.html">Vector&lt;double&gt;</a> &gt;   &amp;value_list)<span class="keyword"> const</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_points = points.<a class="code" href="classVector.html#a8005bf1ec399c608c3755c1d22960add">size</a>();</div><div class="line"></div><div class="line">    <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a> (value_list.size() == n_points,</div><div class="line">            <a class="code" href="group__Exceptions.html#ga6060b2304b8600f5efa0d31eeda0207d">ExcDimensionMismatch</a> (value_list.size(), n_points));</div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p=0; p&lt;n_points; ++p)</div><div class="line">      RightHandSide&lt;dim&gt;::vector_value (points[p],</div><div class="line">                                        value_list[p]);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  ElasticProblem&lt;dim&gt;::ElasticProblem ()</div><div class="line">    :</div><div class="line">    mpi_communicator (MPI_COMM_WORLD),</div><div class="line">    <a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a> (<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>(mpi_communicator)),</div><div class="line">    <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> (<a class="code" href="namespaceUtilities.html">Utilities</a>::MPI::<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>(mpi_communicator)),</div><div class="line">    pcout (<a class="code" href="namespacestd.html">std</a>::cout,</div><div class="line">           (<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a> == 0)),</div><div class="line">    dof_handler (triangulation),</div><div class="line">    fe (<a class="code" href="classFE__Q.html">FE_Q</a>&lt;dim&gt;(1), dim)</div><div class="line">  {}</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  ElasticProblem&lt;dim&gt;::~ElasticProblem ()</div><div class="line">  {</div><div class="line">    dof_handler.<a class="code" href="classDoFHandler.html#ad316958f8045d9a48094335b23a03a53">clear</a> ();</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::setup_system ()</div><div class="line">  {</div><div class="line">    <a class="code" href="namespaceGridTools.html#a5b4706b77042db5437cf81fd13e62b20">GridTools::partition_triangulation</a> (n_mpi_processes, triangulation);</div><div class="line"></div><div class="line">    dof_handler.<a class="code" href="classDoFHandler.html#a553ca864aaf70330d9be86bc78f36d1e">distribute_dofs</a> (fe);</div><div class="line">    <a class="code" href="namespaceDoFRenumbering.html#a442e0991d482f8d208069945d4cee508">DoFRenumbering::subdomain_wise</a> (dof_handler);</div><div class="line"></div><div class="line">    hanging_node_constraints.clear ();</div><div class="line">    <a class="code" href="group__constraints.html#ga3eaa31a679484e80c193e74e8a967dc8">DoFTools::make_hanging_node_constraints</a> (dof_handler,</div><div class="line">                                             hanging_node_constraints);</div><div class="line">    hanging_node_constraints.close ();</div><div class="line"></div><div class="line">    <a class="code" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> dsp(dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>(), dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>());</div><div class="line">    <a class="code" href="group__constraints.html#ga38d88a1a559e9fc65d60f3e168921ba5">DoFTools::make_sparsity_pattern</a> (dof_handler, dsp,</div><div class="line">                                     hanging_node_constraints,</div><div class="line">                                     <span class="keyword">false</span>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> std::vector&lt;IndexSet&gt; locally_owned_dofs_per_proc = <a class="code" href="namespaceDoFTools.html#a6985533ce0fbf92a5a2027191e90c90b">DoFTools::locally_owned_dofs_per_subdomain</a>(dof_handler);</div><div class="line">    <span class="keyword">const</span> <a class="code" href="classIndexSet.html">IndexSet</a> locally_owned_dofs = locally_owned_dofs_per_proc[<a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>];</div><div class="line"></div><div class="line">    system_matrix.reinit (locally_owned_dofs,</div><div class="line">                          locally_owned_dofs,</div><div class="line">                          dsp,</div><div class="line">                          mpi_communicator);</div><div class="line"></div><div class="line">    solution.<a class="code" href="classVector.html#ac4a4dbef7dd65ef8ad35ae56b57d7c05">reinit</a> (locally_owned_dofs, mpi_communicator);</div><div class="line">    system_rhs.reinit (locally_owned_dofs, mpi_communicator);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::assemble_system ()</div><div class="line">  {</div><div class="line">    <a class="code" href="classQGauss.html">QGauss&lt;dim&gt;</a>  quadrature_formula(2);</div><div class="line">    <a class="code" href="classFEValues.html">FEValues&lt;dim&gt;</a> fe_values (fe, quadrature_formula,</div><div class="line">                             <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa4057ca2f127aa619c65886a9d3ad4aea">update_values</a>   | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52facbcc430975fa6af05f75ca786dc6fe20">update_gradients</a> |</div><div class="line">                             <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fad5c9ff886b9615349a5d04a6c782df4a">update_quadrature_points</a> | <a class="code" href="group__feaccess.html#ggaa94b67d2fdcc390690c523f28019e52fa714204722e9eeb43aadbd0d5ddc48c85">update_JxW_values</a>);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>   dofs_per_cell = fe.<a class="code" href="classFiniteElementData.html#ae2fa3b8d578ba488b4f37061bb0278bb">dofs_per_cell</a>;</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span>   n_q_points    = quadrature_formula.size();</div><div class="line"></div><div class="line">    <a class="code" href="classFullMatrix.html">FullMatrix&lt;double&gt;</a>   <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#aa57fdeca62a0708d77768a3bb2aeb826">cell_matrix</a> (dofs_per_cell, dofs_per_cell);</div><div class="line">    <a class="code" href="classVector.html">Vector&lt;double&gt;</a>       cell_rhs (dofs_per_cell);</div><div class="line"></div><div class="line">    std::vector&lt;types::global_dof_index&gt; local_dof_indices (dofs_per_cell);</div><div class="line"></div><div class="line">    std::vector&lt;double&gt;     lambda_values (n_q_points);</div><div class="line">    std::vector&lt;double&gt;     mu_values (n_q_points);</div><div class="line"></div><div class="line">    <a class="code" href="classConstantFunction.html">ConstantFunction&lt;dim&gt;</a> lambda(1.), <a class="code" href="namespacemu.html">mu</a>(1.);</div><div class="line"></div><div class="line">    RightHandSide&lt;dim&gt;      right_hand_side;</div><div class="line">    std::vector&lt;Vector&lt;double&gt; &gt; rhs_values (n_q_points,</div><div class="line">                                             <a class="code" href="classVector.html">Vector&lt;double&gt;</a>(dim));</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keyword">typename</span> <a class="code" href="classDoFHandler.html">DoFHandler&lt;dim&gt;::active_cell_iterator</a></div><div class="line">    cell = dof_handler.<a class="code" href="classDoFHandler.html#a1a36dbbb4c54a7038c60ee9c8eab369a">begin_active</a>(),</div><div class="line">    endc = dof_handler.<a class="code" href="classDoFHandler.html#a7b510a66ee9ea25720f64220496126ec">end</a>();</div><div class="line">    <span class="keywordflow">for</span> (; cell!=endc; ++cell)</div><div class="line">      <span class="keywordflow">if</span> (cell-&gt;subdomain_id() == <a class="code" href="namespaceUtilities_1_1MPI.html#a895dcd8223a0ee6f0e6a80b80e2d5982">this_mpi_process</a>)</div><div class="line">        {</div><div class="line">          <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#aa57fdeca62a0708d77768a3bb2aeb826">cell_matrix</a> = 0;</div><div class="line">          cell_rhs = 0;</div><div class="line"></div><div class="line">          fe_values.reinit (cell);</div><div class="line"></div><div class="line">          lambda.value_list (fe_values.get_quadrature_points(), lambda_values);</div><div class="line">          <a class="code" href="namespacemu.html">mu</a>.value_list     (fe_values.get_quadrature_points(), mu_values);</div><div class="line"></div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i=0; i&lt;dofs_per_cell; ++i)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span></div><div class="line">              component_i = fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> j=0; j&lt;dofs_per_cell; ++j)</div><div class="line">                {</div><div class="line">                  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span></div><div class="line">                  component_j = fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(j).first;</div><div class="line"></div><div class="line">                  <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point=0; q_point&lt;n_q_points;</div><div class="line">                       ++q_point)</div><div class="line">                    {</div><div class="line">                      <a class="code" href="namespaceLocalIntegrators_1_1Advection.html#aa57fdeca62a0708d77768a3bb2aeb826">cell_matrix</a>(i,j)</div><div class="line">                      +=</div><div class="line">                        (</div><div class="line">                          (fe_values.shape_grad(i,q_point)[component_i] *</div><div class="line">                           fe_values.shape_grad(j,q_point)[component_j] *</div><div class="line">                           lambda_values[q_point])</div><div class="line">                          +</div><div class="line">                          (fe_values.shape_grad(i,q_point)[component_j] *</div><div class="line">                           fe_values.shape_grad(j,q_point)[component_i] *</div><div class="line">                           mu_values[q_point])</div><div class="line">                          +</div><div class="line">                          ((component_i == component_j) ?</div><div class="line">                           (fe_values.shape_grad(i,q_point) *</div><div class="line">                            fe_values.shape_grad(j,q_point) *</div><div class="line">                            mu_values[q_point])  :</div><div class="line">                           0)</div><div class="line">                        )</div><div class="line">                        *</div><div class="line">                        fe_values.JxW(q_point);</div><div class="line">                    }</div><div class="line">                }</div><div class="line">            }</div><div class="line"></div><div class="line">          right_hand_side.vector_value_list (fe_values.get_quadrature_points(),</div><div class="line">                                             rhs_values);</div><div class="line">          <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i=0; i&lt;dofs_per_cell; ++i)</div><div class="line">            {</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span></div><div class="line">              component_i = fe.<a class="code" href="classFiniteElement.html#a86644fe67824373cd51e9ff7fca94f8c">system_to_component_index</a>(i).first;</div><div class="line"></div><div class="line">              <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> q_point=0; q_point&lt;n_q_points; ++q_point)</div><div class="line">                cell_rhs(i) += fe_values.shape_value(i,q_point) *</div><div class="line">                               rhs_values[q_point](component_i) *</div><div class="line">                               fe_values.JxW(q_point);</div><div class="line">            }</div><div class="line"></div><div class="line">          cell-&gt;get_dof_indices (local_dof_indices);</div><div class="line">          hanging_node_constraints</div><div class="line">          .distribute_local_to_global(cell_matrix, cell_rhs,</div><div class="line">                                      local_dof_indices,</div><div class="line">                                      system_matrix, system_rhs);</div><div class="line">        }</div><div class="line"></div><div class="line">    system_matrix.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line">    system_rhs.compress(<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae1077e8dbf4afea5d2df8c8b723c0708">VectorOperation::add</a>);</div><div class="line"></div><div class="line">    std::map&lt;types::global_dof_index,double&gt; boundary_values;</div><div class="line">    <a class="code" href="namespaceVectorTools.html#a187aeb575be07bc47cb3dea1a47aaf88">VectorTools::interpolate_boundary_values</a> (dof_handler,</div><div class="line">                                              0,</div><div class="line">                                              <a class="code" href="classZeroFunction.html">ZeroFunction&lt;dim&gt;</a>(dim),</div><div class="line">                                              boundary_values);</div><div class="line">    <a class="code" href="namespaceMatrixTools.html#a9ad0eb7a8662628534586716748d62fb">MatrixTools::apply_boundary_values</a> (boundary_values,</div><div class="line">                                        system_matrix,</div><div class="line">                                        solution,</div><div class="line">                                        system_rhs,</div><div class="line">                                        <span class="keyword">false</span>);</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> ElasticProblem&lt;dim&gt;::solve ()</div><div class="line">  {</div><div class="line">    <a class="code" href="classSolverControl.html">SolverControl</a>           solver_control (solution.<a class="code" href="classVector.html#a8005bf1ec399c608c3755c1d22960add">size</a>(),</div><div class="line">                                            1<a class="code" href="namespacePhysics_1_1Elasticity_1_1Kinematics.html#a9587d5229555daa5b1fa1ba2f8a40adb">e</a>-8*system_rhs.l2_norm());</div><div class="line">    <a class="code" href="classPETScWrappers_1_1SolverCG.html">PETScWrappers::SolverCG</a> cg (solver_control,</div><div class="line">                                mpi_communicator);</div><div class="line"></div><div class="line">    <a class="code" href="classPETScWrappers_1_1PreconditionBlockJacobi.html">PETScWrappers::PreconditionBlockJacobi</a> preconditioner(system_matrix);</div><div class="line"></div><div class="line">    cg.solve (system_matrix, solution, system_rhs,</div><div class="line">              preconditioner);</div><div class="line"></div><div class="line">    <a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution (solution);</div><div class="line"></div><div class="line">    hanging_node_constraints.distribute (localized_solution);</div><div class="line"></div><div class="line">    solution = localized_solution;</div><div class="line"></div><div class="line">    <span class="keywordflow">return</span> solver_control.last_step();</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::refine_grid ()</div><div class="line">  {</div><div class="line">    <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution (solution);</div><div class="line"></div><div class="line">    <a class="code" href="classVector.html">Vector&lt;float&gt;</a> local_error_per_cell (triangulation.n_active_cells());</div><div class="line">    <a class="code" href="classKellyErrorEstimator.html#a971b0bfe57fa21867ed3c06794487e4b">KellyErrorEstimator&lt;dim&gt;::estimate</a> (dof_handler,</div><div class="line">                                        <a class="code" href="classQGauss.html">QGauss&lt;dim-1&gt;</a>(2),</div><div class="line">                                        <span class="keyword">typename</span> <a class="code" href="structFunctionMap.html#a6bb95bc991dd3337330f1c725f59b008">FunctionMap&lt;dim&gt;::type</a>(),</div><div class="line">                                        localized_solution,</div><div class="line">                                        local_error_per_cell,</div><div class="line">                                        <a class="code" href="classComponentMask.html">ComponentMask</a>(),</div><div class="line">                                        <span class="keyword">nullptr</span>,</div><div class="line">                                        <a class="code" href="classMultithreadInfo.html#ad0b84ae105b385b88bdd4bfc0c530995">MultithreadInfo::n_threads</a>(),</div><div class="line">                                        this_mpi_process);</div><div class="line"></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_local_cells</div><div class="line">      = <a class="code" href="namespaceGridTools.html#a8c212a30784bec20b1ae13fad3fd579c">GridTools::count_cells_with_subdomain_association</a> (triangulation,</div><div class="line">                                                           this_mpi_process);</div><div class="line">    <a class="code" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a></div><div class="line">    distributed_all_errors (mpi_communicator,</div><div class="line">                            triangulation.n_active_cells(),</div><div class="line">                            n_local_cells);</div><div class="line"></div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> i=0; i&lt;local_error_per_cell.size(); ++i)</div><div class="line">      <span class="keywordflow">if</span> (local_error_per_cell(i) != 0)</div><div class="line">        distributed_all_errors(i) = local_error_per_cell(i);</div><div class="line">    distributed_all_errors.compress (<a class="code" href="structVectorOperation.html#a40c50779cd14ba89bbf0bd9b4561964cae5042eefddc828c7c31e1e8e26da8b09">VectorOperation::insert</a>);</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;float&gt;</a> localized_all_errors (distributed_all_errors);</div><div class="line"></div><div class="line">    <a class="code" href="namespaceGridRefinement.html#a2500638aae40fe3bfbf094754645dc57">GridRefinement::refine_and_coarsen_fixed_number</a> (triangulation,</div><div class="line">                                                     localized_all_errors,</div><div class="line">                                                     0.3, 0.03);</div><div class="line">    triangulation.execute_coarsening_and_refinement ();</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::output_results (<span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle)<span class="keyword"> const</span></div><div class="line"><span class="keyword">  </span>{</div><div class="line">    <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> localized_solution (solution);</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (this_mpi_process == 0)</div><div class="line">      {</div><div class="line">        std::ostringstream filename;</div><div class="line">        filename &lt;&lt; <span class="stringliteral">&quot;solution-&quot;</span> &lt;&lt; cycle &lt;&lt; <span class="stringliteral">&quot;.vtk&quot;</span>;</div><div class="line"></div><div class="line">        std::ofstream output (filename.str().c_str());</div><div class="line"></div><div class="line">        <a class="code" href="classDataOut.html">DataOut&lt;dim&gt;</a> data_out;</div><div class="line">        data_out.<a class="code" href="classDataOut__DoFData.html#ac1eb26168177faa30ffbcf9cbb9c3cd5">attach_dof_handler</a> (dof_handler);</div><div class="line"></div><div class="line">        std::vector&lt;std::string&gt; solution_names;</div><div class="line">        <span class="keywordflow">switch</span> (dim)</div><div class="line">          {</div><div class="line">          <span class="keywordflow">case</span> 1:</div><div class="line">            solution_names.push_back (<span class="stringliteral">&quot;displacement&quot;</span>);</div><div class="line">            <span class="keywordflow">break</span>;</div><div class="line">          <span class="keywordflow">case</span> 2:</div><div class="line">            solution_names.push_back (<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">            solution_names.push_back (<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">            <span class="keywordflow">break</span>;</div><div class="line">          <span class="keywordflow">case</span> 3:</div><div class="line">            solution_names.push_back (<span class="stringliteral">&quot;x_displacement&quot;</span>);</div><div class="line">            solution_names.push_back (<span class="stringliteral">&quot;y_displacement&quot;</span>);</div><div class="line">            solution_names.push_back (<span class="stringliteral">&quot;z_displacement&quot;</span>);</div><div class="line">            <span class="keywordflow">break</span>;</div><div class="line">          <span class="keywordflow">default</span>:</div><div class="line">            <a class="code" href="group__Exceptions.html#ga70a0bb353656e704acf927945277bbc6">Assert</a> (<span class="keyword">false</span>, <a class="code" href="group__Exceptions.html#ga31978c026b8b6b5116df30b8e748f6b7">ExcInternalError</a>());</div><div class="line">          }</div><div class="line"></div><div class="line">        data_out.add_data_vector (localized_solution, solution_names);</div><div class="line"></div><div class="line">        std::vector&lt;unsigned int&gt; partition_int (triangulation.n_active_cells());</div><div class="line">        <a class="code" href="namespaceGridTools.html#ae0cb61bdd7e17c6b1589bdd16891e561">GridTools::get_subdomain_association</a> (triangulation, partition_int);</div><div class="line"></div><div class="line">        <span class="keyword">const</span> <a class="code" href="classVector.html">Vector&lt;double&gt;</a> partitioning(partition_int.begin(),</div><div class="line">                                          partition_int.end());</div><div class="line"></div><div class="line">        data_out.add_data_vector (partitioning, <span class="stringliteral">&quot;partitioning&quot;</span>);</div><div class="line"></div><div class="line">        data_out.build_patches ();</div><div class="line">        data_out.write_vtk (output);</div><div class="line">      }</div><div class="line">  }</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  <span class="keyword">template</span> &lt;<span class="keywordtype">int</span> dim&gt;</div><div class="line">  <span class="keywordtype">void</span> ElasticProblem&lt;dim&gt;::run ()</div><div class="line">  {</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> cycle=0; cycle&lt;10; ++cycle)</div><div class="line">      {</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;Cycle &quot;</span> &lt;&lt; cycle &lt;&lt; <span class="charliteral">&#39;:&#39;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (cycle == 0)</div><div class="line">          {</div><div class="line">            <a class="code" href="namespaceGridGenerator.html#acea0cbcd68e52ce8113d1134b87de403">GridGenerator::hyper_cube</a> (triangulation, -1, 1);</div><div class="line">            triangulation.refine_global (3);</div><div class="line">          }</div><div class="line">        <span class="keywordflow">else</span></div><div class="line">          refine_grid ();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of active cells:       &quot;</span></div><div class="line">              &lt;&lt; triangulation.n_active_cells()</div><div class="line">              &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        setup_system ();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Number of degrees of freedom: &quot;</span></div><div class="line">              &lt;&lt; dof_handler.<a class="code" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">n_dofs</a>()</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot; (by partition:&quot;</span>;</div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> p=0; p&lt;<a class="code" href="namespaceUtilities_1_1MPI.html#ac26de0c059200523177bb1d92cc25d00">n_mpi_processes</a>; ++p)</div><div class="line">          pcout &lt;&lt; (p==0 ? <span class="charliteral">&#39; &#39;</span> : <span class="charliteral">&#39;+&#39;</span>)</div><div class="line">                &lt;&lt; (<a class="code" href="namespaceDoFTools.html#a2dde9ded6d7cbc9962547356f37e5f72">DoFTools::</a></div><div class="line"><a class="code" href="namespaceDoFTools.html#a2dde9ded6d7cbc9962547356f37e5f72">                    count_dofs_with_subdomain_association</a> (dof_handler,</div><div class="line">                                                           p));</div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;)&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        assemble_system ();</div><div class="line">        <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n_iterations = solve ();</div><div class="line"></div><div class="line">        pcout &lt;&lt; <span class="stringliteral">&quot;   Solver converged in &quot;</span> &lt;&lt; n_iterations</div><div class="line">              &lt;&lt; <span class="stringliteral">&quot; iterations.&quot;</span> &lt;&lt; std::endl;</div><div class="line"></div><div class="line">        output_results (cycle);</div><div class="line">      }</div><div class="line">  }</div><div class="line">}</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keywordtype">int</span> main (<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> **argv)</div><div class="line">{</div><div class="line">  <span class="keywordflow">try</span></div><div class="line">    {</div><div class="line">      <span class="keyword">using namespace </span><a class="code" href="namespacedealii.html">dealii</a>;</div><div class="line">      <span class="keyword">using namespace </span>Step17;</div><div class="line"></div><div class="line">      <a class="code" href="classUtilities_1_1MPI_1_1MPI__InitFinalize.html">Utilities::MPI::MPI_InitFinalize</a> mpi_initialization(argc, argv, 1);</div><div class="line"></div><div class="line">      ElasticProblem&lt;2&gt; elastic_problem;</div><div class="line">      elastic_problem.run ();</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (std::exception &amp;exc)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Exception on processing: &quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; exc.what() &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line"></div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line">  <span class="keywordflow">catch</span> (...)</div><div class="line">    {</div><div class="line">      std::cerr &lt;&lt; std::endl &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;Unknown exception!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Aborting!&quot;</span> &lt;&lt; std::endl</div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;----------------------------------------------------&quot;</span></div><div class="line">                &lt;&lt; std::endl;</div><div class="line">      <span class="keywordflow">return</span> 1;</div><div class="line">    }</div><div class="line"></div><div class="line">  <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --> </div></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.13-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
